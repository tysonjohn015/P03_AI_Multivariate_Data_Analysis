{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Sommaire",
      "title_sidebar": "Sommaire",
      "toc_cell": true,
      "toc_position": {
        "height": "550px",
        "left": "25px",
        "top": "111.133px",
        "width": "290.867px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "P3_01_Script.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jTE9W9jyXyqG"
      ]
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
     {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tysonjohn015/P03_AI_Multivariate_Data_Analysis/blob/main/P3_01_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVdunCW5Xynj"
      },
      "source": [
        "import os\n",
        "import urllib\n",
        "import zipfile\n",
        "import re\n",
        "import itertools\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.patches as mpatches\n",
        "import seaborn as sns\n",
        "\n",
        "import plotly.offline as pyo\n",
        "import plotly.express as px\n",
        "\n",
        "import scipy.stats as st\n",
        "from sklearn import decomposition\n",
        "from sklearn import preprocessing\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import missingno as msno\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from IPython.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A78gpJFkXyn0"
      },
      "source": [
        "# Initialisation des options d'affichage\n",
        "pd.set_option('display.max_columns', None)\n",
        "sns.set(font_scale=1.2)\n",
        "pyo.init_notebook_mode()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZsWxvkEXyoV"
      },
      "source": [
        "def print_md(text):\n",
        "    \"\"\"Affichage d'un text en Markdown\"\"\"\n",
        "    display(Markdown(text))\n",
        "\n",
        "def print_md_df_shape(df, df_origin=None):\n",
        "    \"\"\"\n",
        "    Affichage des dimension de l'échantillon.\n",
        "    Comparison avec l'échnatillon d'origine.\n",
        "    \"\"\"\n",
        "    \n",
        "    text = \"Dimensions de l'échantillon :\\n\"\n",
        "    \n",
        "    if df_origin is not None:\n",
        "        origin_rows_pct = int(df.shape[0] * 100 / df_origin.shape[0])\n",
        "        origin_cols_pct = int(df.shape[1] * 100 / df_origin.shape[1])\n",
        "        \n",
        "        text += f\"- **Nombre d'individus** : {df.shape[0]} ({origin_rows_pct}% du dataset complet)\\n\"\n",
        "        text += f\"- **Nombre de variables** : {df.shape[1]} ({origin_cols_pct}% du dataset complet)\\n\"\n",
        "    else:\n",
        "        text += f\"- **Nombre d'individus** : {df.shape[0]}\\n\"\n",
        "        text += f\"- **Nombre de variables** : {df.shape[1]}\\n\"\n",
        "\n",
        "    print_md(text)\n",
        "\n",
        "def get_cat_var_emp_dist_df(df, c, k=None):\n",
        "    \"\"\"Renvoie la distribution empirique d'une variable de type catégorie\"\"\"\n",
        "    if k is None:\n",
        "        effectifs = list(df[c].value_counts().values)\n",
        "        labels = df[c].value_counts().index.to_list()\n",
        "    else:\n",
        "        effectifs = list(df[c].value_counts().iloc[:k].values)\n",
        "        labels = df[c].value_counts().iloc[:k].index.to_list()\n",
        "        \n",
        "        # Si il reste des modalités, on les aggrège ensemble\n",
        "        effectif = df[c].value_counts().iloc[k:].sum()\n",
        "        if effectif > 0:\n",
        "            effectifs.append(effectif)\n",
        "            labels.append(\"autres\")\n",
        "\n",
        "    df_tmp = pd.DataFrame({\n",
        "        c: labels,\n",
        "        \"effectif\": effectifs\n",
        "    })\n",
        "    \n",
        "    df_tmp[\"f\"] = df_tmp[\"effectif\"] / df[c].count()\n",
        "    df_tmp[\"F\"] = df_tmp[\"f\"].cumsum()\n",
        "    \n",
        "    return df_tmp\n",
        "\n",
        "def categ_plot_pie_chart(df, c, k):\n",
        "    \"\"\"Affiche un pie chart de la distribution d'une variable de type catégorie\"\"\"\n",
        "    df_tmp = get_cat_var_emp_dist_df(df, c, k)\n",
        "\n",
        "    # On crée le pie chart\n",
        "    fig = px.pie(\n",
        "        df_tmp,\n",
        "        values=\"effectif\",\n",
        "        names=c,\n",
        "        title=f\"Distribution de la variable {c}\"\n",
        "    )\n",
        "    fig.update_traces(textinfo='percent+label')\n",
        "    # fig.show()\n",
        "    return HTML(fig.to_html())\n",
        "\n",
        "def categ_plot_bar_chart(df, c, k, log=False):\n",
        "    \"\"\"Affiche un bar chart de la distribution d'une variable de type catégorie\"\"\"\n",
        "    df_tmp = get_cat_var_emp_dist_df(df, c, k)\n",
        "\n",
        "    # On crée le bar chart\n",
        "    fig = px.bar(\n",
        "        df_tmp,\n",
        "        y='effectif',\n",
        "        x=c,\n",
        "        text='effectif',\n",
        "        title=f\"Distribution de la variable {c}\",\n",
        "        log_y=log\n",
        "    )\n",
        "    fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n",
        "    # fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n",
        "    # fig.show()\n",
        "    return HTML(fig.to_html())\n",
        "\n",
        "def num_categ_box_chart(\n",
        "        df_clean,\n",
        "        num_col,\n",
        "        cat_col,\n",
        "        title=\"\",\n",
        "        xlabel=\"\",\n",
        "        ylabel=\"\",\n",
        "        showfliers=True,\n",
        "        rotation=0\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Affiche un box chart d'une variable numérique en fonction des modalités\n",
        "    d\"une variable de type catégorie.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(16, 16))\n",
        "\n",
        "    meanprops = {\n",
        "        'marker':'o',\n",
        "        'markeredgecolor':'black',\n",
        "        'markerfacecolor':'firebrick'\n",
        "    }\n",
        "\n",
        "    sns.boxplot(\n",
        "        data=df_clean.sort_values(cat_col),\n",
        "        x=cat_col,\n",
        "        y=num_col,\n",
        "        showfliers=showfliers,\n",
        "        showmeans=True,\n",
        "        meanprops=meanprops,\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    \n",
        "    if rotation > 0:\n",
        "        ax.set_xticklabels(ax.get_xticklabels(), rotation=rotation)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZvz3-anXyos"
      },
      "source": [
        "# Présentation générale du jeu de données de Open Food Facts\n",
        "\n",
        "L'agence Santé publique France souhaite rendre les données de santé publique plus accessibles, pour qu’elles soient utilisables par ses agents.\n",
        "\n",
        "Pour cela, nous allons anlyser un jeu de données fourni gratuitement par [Open Food Facts](https://world.openfoodfacts.org/who-we-are).\n",
        "Il s'agit d'un projet collaboratif qui vise à collecter des données sur des produits alimentaires que l'on retrouve en grande surface. Le projet a été initié en France mais il est depuis devenu international.\n",
        "\n",
        "Le projet possède une page wiki avec des nombreuses informations sur son jeu de données : [wiki](https://wiki.openfoodfacts.org/Main_Page).\n",
        "\n",
        "## Téléchargement du jeu de données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akJ8fvsfGVdn"
      },
      "source": [
        "# # Run this cell and select the kaggle.json file downloaded\n",
        "# # from the Kaggle account settings page.\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDLXrqcNGlU7"
      },
      "source": [
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QITWh1N5GoqG"
      },
      "source": [
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KixiXyagGy7Y"
      },
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLSrgfqTYZeI"
      },
      "source": [
        "**Téléchargement du jeu de données**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hyJ4Rb5HMwH"
      },
      "source": [
        "# Copy the stackoverflow data set locally.\n",
        "!kaggle datasets download -d tysonjohn/openfoodfacts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGVny0ZqSQyC"
      },
      "source": [
        "!unzip openfoodfacts.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ov0F9yXXyo7"
      },
      "source": [
        "# Chargement des données dans un DataFrame\n",
        "df = pd.read_csv('fr.openfoodfacts.org.products.csv', sep='\\t')\n",
        "\n",
        "print_md_df_shape(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCO8-arrXypK"
      },
      "source": [
        "## Premières lignes\n",
        "\n",
        "Observons les premières lignes de notre jeu de données :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo04q0FmXypV"
      },
      "source": [
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHGmZDabXypq"
      },
      "source": [
        "## Valeurs manquantes\n",
        "\n",
        "En observant ces premières lignes, on remarque que beaucoup de colonnes sont remplies de valeurs manquantes (NaN)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek2oKpirXyp2"
      },
      "source": [
        "# On calcul le pourcentage de valeurs manquantes par variable\n",
        "df_na = df.isna().sum() * 100 / df.shape[0]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "\n",
        "# On affiche l'histogramme des valeurs manquantes\n",
        "hist = df_na.hist(bins=20, ax=ax)\n",
        "\n",
        "ax.set_title(\"Distribution des valeurs manquantes des variables\")\n",
        "ax.set_xlabel(\"Pourcentage de valeurs manquantes\")\n",
        "ax.set_ylabel(\"Nombre de variables\")\n",
        "\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "# On crée un code couleur qui fait preuve d'affordance\n",
        "colors = [\"mediumseagreen\", \"gold\", \"lightcoral\"]\n",
        "nodes = [0.0, 0.5, 1.0]\n",
        "mycmap = LinearSegmentedColormap.from_list(\"mycmap\", list(zip(nodes, colors)))\n",
        "\n",
        "for i, p in enumerate(hist.patches):\n",
        "  plt.setp(p, 'facecolor', mycmap(i/20))\n",
        "\n",
        "# On ajoute la valeur de la première classe\n",
        "h = hist.get_children()[0].get_height()\n",
        "ax.text(1.2, h + 0.6, f\"{int(h)}\")\n",
        "\n",
        "# On ajoute la valeur de la dernière classe\n",
        "h = hist.get_children()[19].get_height()\n",
        "ax.text(96, h + 0.6, f\"{int(h)}\")\n",
        "\n",
        "xticks = list(range(0, 105, 5))\n",
        "ax.set_xticks(xticks)\n",
        "ax.set_xticklabels([str(x) for x in xticks])\n",
        "\n",
        "# plt.savefig('distribution valeurs manquantes.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calcul du nombre total de valeurs manquates\n",
        "na_nb = df.isna().sum().sum()\n",
        "\n",
        "text = f\"- **Valeurs manquantes** : {na_nb} sur {df.size} (soit {na_nb * 100 / df.size:0.2f}%)\\n\"\n",
        "\n",
        "print_md(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyIdZCIklfDa"
      },
      "source": [
        "# ToDeleteCell: to analysing the above code\n",
        "for i, p in enumerate(hist.patches):\n",
        "  print(i, '**', p,'***', mycmap(i/20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTE9W9jyXyqG"
      },
      "source": [
        "**On** a 108 variables (58% des variables) qui ont plus de 95% de valeurs manquantes. Seules 16 variables (8,6% des variables) ont moins de 5% de valeurs manquantes.\n",
        "\n",
        "Pour rappel, ce jeu de données est libre d'accès et peut être rempli par n'importe qui. De plus, certaines variables sont rarement disponibles sur les emballages des produits comme par exemple l'indice glycémique.\n",
        "\n",
        "## Colonnes doublons\n",
        "\n",
        "En observant chaques variables, on remarque que certaines ont le même nom racine comme :\n",
        "- `countries`\n",
        "- `countries_tags`\n",
        "- `countries_fr`\n",
        "\n",
        "Ou encore :\n",
        "- `last_modified_t`:\n",
        "- `last_modified_datetime`\n",
        "\n",
        "Ces variables représentent la même information mais sous un format différent ([en savoir plus](https://static.openfoodfacts.org/data/data-fields.txt))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw2pMtraXyqg"
      },
      "source": [
        "## Suppression de variables inutiles\n",
        "\n",
        "Commençons par supprimer les variables qui ont plus de 95% de valeurs manquantes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhDB196EXyqo"
      },
      "source": [
        "# Initializing\n",
        "rows = []\n",
        "df_clean = df\n",
        "\n",
        "# On va analyser chaque colonne\n",
        "for c in df_clean.columns:\n",
        "    row = {}\n",
        "    row[\"Valeurs manquantes\"] = df_clean[c].isna().sum() * 100 / df_clean.shape[0]\n",
        "    row[\"Valeurs nulles\"] = ((df_clean[c] == 0) | (df_clean[c] == \"\")).sum() * 100 / df_clean.shape[0]\n",
        "    row[\"Valeurs non nulles\"] = 100 - (row[\"Valeurs manquantes\"] + row[\"Valeurs nulles\"])\n",
        "    \n",
        "    rows.append(row)\n",
        "\n",
        "# On met le tout dans un DataFrame et on filtre par valeurs manquantes\n",
        "df_columns = pd.DataFrame(rows, index=df_clean.columns)\n",
        "df_columns = df_columns.sort_values(\"Valeurs manquantes\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_8kbQW3pjP-"
      },
      "source": [
        "# ToDeleteCell : \n",
        "# df_columns[df_columns['Valeurs manquantes'] >= 95].shape\n",
        "df_columns[df_columns['Valeurs manquantes'] >= 95].index.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mvMqd_wXyqu"
      },
      "source": [
        "# On récupère la liste des variables à supprimer\n",
        "cols_to_drop = df_columns[df_columns[\"Valeurs manquantes\"] > 95].index.to_list()\n",
        "\n",
        "# On supprime les variables\n",
        "df_clean = df_clean.drop(columns=cols_to_drop)\n",
        "df_columns = df_columns.drop(index=cols_to_drop)\n",
        "\n",
        "print_md_df_shape(df_clean, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WS5ae7cXyq0"
      },
      "source": [
        "Regardons les variables ayant plus de 50% de valeurs manquantes :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "0Ev7N-AoXyq4"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(16,32))\n",
        "\n",
        "# On affiche les indicateurs statistiques des variables ayant plus de 50% de valeurs manquantes\n",
        "df_columns[df_columns[\"Valeurs manquantes\"] > 50].plot.barh(\n",
        "    stacked=True,\n",
        "    ax=ax,\n",
        "    fontsize=16,\n",
        "    color={\n",
        "        \"Valeurs manquantes\": \"lightcoral\",\n",
        "        \"Valeurs nulles\": \"gold\",\n",
        "        \"Valeurs non nulles\": \"mediumseagreen\",\n",
        "    }\n",
        ")\n",
        "\n",
        "# On trace une ligne indiquant 75% de la population\n",
        "ax.axvline(75, color=\"grey\", ls=\"--\", alpha=0.6, label=\"75% des individus\")\n",
        "\n",
        "ax.set_title(\"Représentation des valeurs manquantes\", fontsize=16)\n",
        "ax.set_xlabel(\"Pourcentage des individus\")\n",
        "\n",
        "plt.legend(loc='upper left', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ7Z2M7mXyrB"
      },
      "source": [
        "Nous allons supprimer les variables qui ont plus de 75% de valeurs manquantes car la grande majorité ne va pas nous intéresser pour l'analyse. Parmis ces variables, nous allons cependant conserver `additives_fr`,`allergens`.\n",
        "\n",
        "Nous constatons aussi que certaines variables ont énromément de valeurs nulles comme `ingredients_from_palm_oil_n` ou encore `ingredients_that_may_be_from_palm_oil_n`. Nous allons supprimer ces variables par la suite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTy2MJMivMXh"
      },
      "source": [
        "# ToDeleteCell\n",
        "df_columns[df_columns[\"Valeurs manquantes\"] <= 75.].index.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Fz_xW7_NXyrI"
      },
      "source": [
        "# On récupère la liste des variables ayant plus de 75% de valeurs manquantes\n",
        "cols_to_drop = df_columns[df_columns[\"Valeurs manquantes\"] > 75.].index.to_list()\n",
        "\n",
        "# Variables que l'on souhaite conserver même si elles ont beaucoup de valeurs manquantes\n",
        "cols_to_keep = [\n",
        "    \"additives_fr\",\n",
        "    \"allergens\"\n",
        "]\n",
        "\n",
        "# Variables à supprimer\n",
        "cols_to_drop = [c for c in cols_to_drop if c not in cols_to_keep]\n",
        "\n",
        "# Suppression des variables\n",
        "df_clean = df_clean.drop(columns=cols_to_drop)\n",
        "df_columns = df_columns.drop(index=cols_to_drop)\n",
        "\n",
        "print_md_df_shape(df_clean, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pUdz6okXyrR"
      },
      "source": [
        "Regardons les variables représentant des catégories de produits :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0XhI2XZXyrX"
      },
      "source": [
        "product_cat_cols = [\n",
        "    \"categories\",\n",
        "    \"categories_tags\",\n",
        "    \"categories_fr\",\n",
        "    \"pnns_groups_1\",\n",
        "    \"pnns_groups_2\",\n",
        "]\n",
        "\n",
        "df_cat = df_clean[product_cat_cols].describe().T\n",
        "df_cat[\"missing values\"] = df_clean[product_cat_cols].isna().sum()\n",
        "\n",
        "display(df_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeftIPV2Xyrl"
      },
      "source": [
        "Supprimons les modalités `unknown` afin qu'elles ne soient plus prises en compte dans le calcul de nos indicateurs statistiques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTORV9KsXyrr"
      },
      "source": [
        "# On commence par supprimer les modalités \"unknown\" pour qu'elles ne soient pas prises en compte\n",
        "df_clean.loc[:, product_cat_cols] = df_clean[product_cat_cols].replace(\"unknown\", np.nan)\n",
        "\n",
        "df_cat = df_clean[product_cat_cols].describe().T\n",
        "df_cat[\"missing values\"] = df_clean[product_cat_cols].isna().sum().values\n",
        "\n",
        "display(df_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZicGGxnXyr7"
      },
      "source": [
        "Les variables `categor.y.ies*` contiennent beaucoup trop de modalités.\n",
        "Nous voudrions pourvoir classer nos produits en quelques dizaines de catégories au maximum.\n",
        "\n",
        "Les variables `pnns_groups_1` et `pnns_groups_2` ont toutes les 2 un nombre raisonnable de modalités."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10g05GfiXyr_"
      },
      "source": [
        "def print_md_dist_emp_pnns_groups(df):\n",
        "    outs = []\n",
        "\n",
        "    for column in [\"pnns_groups_1\", \"pnns_groups_2\"]:\n",
        "        outs.append(widgets.Output())\n",
        "\n",
        "        with outs[-1]:\n",
        "            df_distCat = get_cat_var_emp_dist_df(df, column)\n",
        "\n",
        "            print_md(f\"<center>Distribution empirique des {df_distCat.shape[0]} principales</br>modalités de {column}</center>\")\n",
        "            display(df_distCat)\n",
        "\n",
        "    hbox = widgets.HBox(outs)\n",
        "    hbox.layout.justify_content=\"space-around\"\n",
        "    \n",
        "    display(hbox)\n",
        "\n",
        "print_md_dist_emp_pnns_groups(df_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StX8p3NzXysE"
      },
      "source": [
        "On remarque qu'il y a des doublons dans les modalités. On peut facilement corriger cela en remplaçant le caractère `-` par un espace et en mettant les premières lettres des modalités en majuscule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWNuZRJxXysI"
      },
      "source": [
        "for c in [\"pnns_groups_1\", \"pnns_groups_2\"]:\n",
        "    df_clean[c] = df_clean[c].transform(lambda x: x.str.replace(\"-\", \" \").str.capitalize())\n",
        "\n",
        "# Combine similar categories\n",
        "df_clean[\"pnns_groups_2\"] = df_clean[\"pnns_groups_2\"].replace({\n",
        "    \"Pizza pies and quiche\": \"Pizza pies and quiches\"\n",
        "})\n",
        "\n",
        "df_clean[\"pnns_groups_2\"] = df_clean[\"pnns_groups_2\"].replace({\n",
        "    \"Legumes\": \"Vegetables\"\n",
        "})\n",
        "\n",
        "print_md_dist_emp_pnns_groups(df_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfxykMYUXysP"
      },
      "source": [
        "`pnns_groups_1` contient 10 catégories de produits explicites et assez générales. `pnns_groups_2` en contient 38 qui sont plus détaillées. Nous allons conserver ces 2 variables.\n",
        "\n",
        "Nous allons filtrer les variables doublons et les variables inutiles (trop de valeurs nulles etc...) :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g1NifcTXysU"
      },
      "source": [
        "# On définit la liste des colonnes à conserver.\n",
        "cols_to_keep = [\n",
        "    \"code\",\n",
        "\n",
        "    \"creator\",\n",
        "    \"last_modified_t\",\n",
        "\n",
        "    \"product_name\",\n",
        "    \"brands\",\n",
        "    \"countries_fr\",\n",
        "    \"pnns_groups_1\",\n",
        "    \"pnns_groups_2\",\n",
        "\n",
        "    \"nutriscore_grade\",\n",
        "    \"nutriscore_score\",\n",
        "\n",
        "    \"ingredients_text\",\n",
        "    \"additives_n\",\n",
        "    \"additives_fr\",\n",
        "    \"allergens\",\n",
        "\n",
        "    \"energy_100g\",\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\"\n",
        "]\n",
        "\n",
        "# On filtre les variables que l'on souhaite conserver\n",
        "df_clean = df_clean[cols_to_keep]\n",
        "\n",
        "print_md_df_shape(df_clean, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmuATMUzXysb"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "tmp = msno.bar(df_clean, ax=ax)\n",
        "ax.set_title(\n",
        "    \"\"\"\n",
        "    Représentation des valeurs présentes par variable.\n",
        "    \"\"\",\n",
        "    fontsize=16)\n",
        "fig.axes[0].set_ylabel(\"Fréquence de valeurs présentes\", fontsize=16)\n",
        "fig.axes[1].set_ylabel(\"Nombre de valeurs présentes\", fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jru9bZGlXysk"
      },
      "source": [
        "## Suppression des doublons dans les individus\n",
        "\n",
        "Commençons par observer les individus qui ont le même code bar :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-X53JWZOAjc"
      },
      "source": [
        "df_clean.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cLxMRRIXysn"
      },
      "source": [
        "# On récupère les doublons\n",
        "doublons_df = df_clean[df_clean.duplicated(\"code\", keep=False)]\n",
        "\n",
        "# On affiche les premiers doublons\n",
        "display(doublons_df.astype({\"code\": str}).sort_values(\"code\").head(8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWlpbD4YXysu"
      },
      "source": [
        "Il s'agit effectivement de doublons.\n",
        "\n",
        "Nous allons conserver les individus ayant la date de mise à jour la plus récente et nous allons compléter leurs valeurs manquantes avec les valeurs de leurs doublons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VALMRWFPXysy"
      },
      "source": [
        "# On met à jour les valeurs manquantes des doublons\n",
        "doublons_df = doublons_df.astype({\"code\": str}).sort_values([\"code\", \"last_modified_t\"]). \\\n",
        "    groupby(\"code\", as_index=False). \\\n",
        "    fillna(method=\"ffill\")\n",
        "\n",
        "# On met à jour les doublons\n",
        "df_clean.update(doublons_df)\n",
        "\n",
        "# On supprime les doublons\n",
        "df_clean = df_clean.sort_values(\"last_modified_t\").drop_duplicates(subset=\"code\")\n",
        "\n",
        "print_md_df_shape(df_clean, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0GzU-uYXys4"
      },
      "source": [
        "## Nettoyage des valeurs du tableau nutritionel\n",
        "\n",
        "Obervons quelques indicateurs statistiques des variables nutritionelles :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70LLYmN3W60b"
      },
      "source": [
        "df_clean['energy_100g'][df_clean['energy_100g'].apply(lambda x: isinstance(x, str))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6K-ngKUUNTk"
      },
      "source": [
        "df_clean['energy_100g'] = pd.to_numeric(df_clean['energy_100g'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5IAXrPCXys7"
      },
      "source": [
        "df_clean.describe().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlQkzEO3XytB"
      },
      "source": [
        "On remarque que certaines valeurs max sont trop grandes. Par exemple, la valeur max de l'énergie au 100g ne devrait pas dépasser les 3700kJ ([en savoir plus](https://fr.wikipedia.org/wiki/Valeur_%C3%A9nerg%C3%A9tique)).\n",
        "\n",
        "Les valeurs aux 100g (excepté `energy_100g` qui est en kJ) ne devrait pas dépasser les 100g.\n",
        "\n",
        "Il ne devrait aussi pas y avoir de valeurs négatives (sauf pour `nutriscore_score`).\n",
        "\n",
        "Commençons par observer certains outliers de `energy_100g` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "h2LNcCHNXyuB"
      },
      "source": [
        "# Exemples de valeurs atypiques\n",
        "display(df_clean[pd.to_numeric(df_clean[\"energy_100g\"]) > 3700].head(2))\n",
        "\n",
        "# Exemples de valeurs aberrantes\n",
        "display(df_clean[pd.to_numeric(df_clean[\"energy_100g\"]) > 4000].head(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89_Odbj_XyvE"
      },
      "source": [
        "Les valeurs affichées ci-dessus contiennent des valeurs atypique qui avoisinent les 3700kJ. Ce sont principalement des huiles qui contiennent 100% de matière grasse. Nous allons donc les conserver.\n",
        "\n",
        "On observe aussi des valeurs beaucoup plus élevées qui sont certainement des valeurs aberrantes.\n",
        "\n",
        "Nous allons clipper les valeurs inférieures à 3800kJ à 3700kJ. Les valeurs supérieures à 3800kJ seront considérés comme des valeurs aberrantes et seront supprimées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzIuYcKMXyvI"
      },
      "source": [
        "cols = [\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "]\n",
        "\n",
        "all_cols = cols + [\"energy_100g\"]\n",
        "\n",
        "# On supprime les valeurs aberrantes de l'énergie\n",
        "df_clean.loc[df_clean[\"energy_100g\"] < 0, all_cols] = np.nan\n",
        "df_clean.loc[df_clean[\"energy_100g\"] >= 3800, all_cols] = np.nan\n",
        "\n",
        "# On clip les valeur atypiques à 3700kJ\n",
        "df_clean[\"energy_100g\"] = df_clean[\"energy_100g\"].clip(upper=3700)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6GzVIBjXyvM"
      },
      "source": [
        "Voyons combien d'individus ont leur tableau nutritionnel complet :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9fsctZcXyvP"
      },
      "source": [
        "def print_md_notna_sizes():\n",
        "    cols = [\n",
        "        \"energy_100g\",\n",
        "        \"fat_100g\",\n",
        "        \"saturated-fat_100g\",\n",
        "        \"carbohydrates_100g\",\n",
        "        \"sugars_100g\",\n",
        "        \"fiber_100g\",\n",
        "        \"proteins_100g\",\n",
        "        \"salt_100g\",\n",
        "    ]\n",
        "\n",
        "    text = \"Voici le nombre d'individus dans les 2 sous-échantilllons suivants :\\n\"\n",
        "\n",
        "    row_nb = df_clean.dropna(subset=cols).shape[0]\n",
        "    row_pct = row_nb * 100 / df_clean.shape[0]\n",
        "    text += f\"- **Individus ayant leur tableau nutritionnel complet** : {row_nb} ({int(row_pct)}% des individus)\\n\"\n",
        "\n",
        "    row_nb = df_clean.dropna(subset=cols + [\"pnns_groups_2\"]).shape[0]\n",
        "    row_pct = row_nb * 100 / df_clean.shape[0]\n",
        "    text += f\"- **Individus ayant leur tableau nutritionnel complet et ayant la catégorie du produit** : {row_nb} ({int(row_pct)}% des individus)\\n\"\n",
        "\n",
        "    print_md(text)\n",
        "\n",
        "print_md_notna_sizes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5JzmG5eXyvT"
      },
      "source": [
        "def print_md_missing_val_nutri_table():\n",
        "    cols = [\n",
        "        \"fat_100g\",\n",
        "        \"carbohydrates_100g\",\n",
        "        \"fiber_100g\",\n",
        "        \"proteins_100g\",\n",
        "    ]\n",
        "\n",
        "    text = \"Nombre d'individus ayant leur valeur de l'énergie aux 100g \"\n",
        "    text += \"et ayant des valeurs manquantes dans leur tableau nutritionnel :\\n\"\n",
        "\n",
        "    tmp = df_clean[df_clean[\"energy_100g\"].notna()][cols].isna().sum(axis=1)\n",
        "    for i in range(4):\n",
        "        tmp2 = (tmp == (i + 1)).sum()\n",
        "        text += f\"- Abscence de {i + 1} valeur(s) : {tmp2} individus\\n\"\n",
        "\n",
        "    print_md(text)\n",
        "\n",
        "print_md_missing_val_nutri_table()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGDdmaJXXyvZ"
      },
      "source": [
        "Grâce à la formule de calcul de l'énergie au 100g, nous allons pouvoir compléter le tableau nutritionnel des individus ayant 1 seule valeur manquante :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ScGxZDrXyvc"
      },
      "source": [
        "cols = [\n",
        "    \"fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "]\n",
        "\n",
        "# On crée un masque des individus ayant 1 seule valeur manquante\n",
        "mask = df_clean[\"energy_100g\"].notna() & (df_clean[cols].isna().sum(axis=1) == 1)\n",
        "\n",
        "# Doubt : col_to_weight how its decided?\n",
        "# On attribut à chaque variable son poids dans le calcul de l'énergie au 100g\n",
        "col_to_weight = {\n",
        "    \"fat_100g\": 37,\n",
        "    \"carbohydrates_100g\": 17,\n",
        "    \"fiber_100g\": 17,\n",
        "    \"proteins_100g\": 8,\n",
        "}\n",
        "\n",
        "for c in col_to_weight:\n",
        "    # On filtre les individus ayant la valeur de la variable c manquante\n",
        "    mask2 = mask & df_clean[c].isna()\n",
        "    \n",
        "    # On enlève c de la formule\n",
        "    ctw = col_to_weight.copy()\n",
        "    # Doubt: what happening below ?\n",
        "    del(ctw[c])\n",
        "    # Doubt: what happening below ?\n",
        "    df_clean.loc[mask2, c] = df_clean.loc[mask2, \"energy_100g\"]\n",
        "    for c2, w in ctw.items():\n",
        "        df_clean.loc[mask2, c] -= df_clean.loc[mask2, c2] * w\n",
        "\n",
        "print_md_missing_val_nutri_table()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H5zt4ixXyvk"
      },
      "source": [
        "Nous allons clipper les valeurs des nutriments comprises entre 100 et 120g à 100g. Les valeurs supérieures à 120g ou négatives seront considérées comme des valeurs aberrantes et seront supprimées.\n",
        "\n",
        "Nous allons aussi supprimer les valeurs des nutriments si leur somme est supérieure à 120g."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UolE6s7muIW_"
      },
      "source": [
        "df_clean[cols] > 120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebU62cHJXyvp"
      },
      "source": [
        "cols = [\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "]\n",
        "\n",
        "all_cols = cols + [\"energy_100g\"]\n",
        "\n",
        "# On supprime les valeurs aberrantes de l'énergie\n",
        "df_clean.loc[df_clean[\"energy_100g\"] < 0, all_cols] = np.nan\n",
        "df_clean.loc[df_clean[\"energy_100g\"] >= 3800, all_cols] = np.nan\n",
        "\n",
        "# On clip les valeur atypiques à 3700kJ\n",
        "df_clean[\"energy_100g\"] = df_clean[\"energy_100g\"].clip(upper=3700)\n",
        "\n",
        "# On supprime les valeurs aberrantes des nutriments\n",
        "df_clean.loc[(df_clean[cols] < 0).sum(axis=1) > 0, all_cols] = np.nan\n",
        "df_clean.loc[(df_clean[cols] > 120).sum(axis=1) > 0, all_cols] = np.nan\n",
        "    \n",
        "# On clip les valeur atypiques à 100g\n",
        "df_clean.loc[:, cols] = df_clean[cols].clip(lower=0, upper=100)\n",
        "\n",
        "cols = [\n",
        "    \"fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "]\n",
        "\n",
        "# On supprime les valeurs aberrantes si la somme des nutriments > 120g\n",
        "df_clean.loc[df_clean[cols].sum(axis=1) > 120, all_cols] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4ZATRS5Xyvu"
      },
      "source": [
        "Nous allons maintenant recalculer l'énergie au 100g grâce aux valeurs des nutriments.\n",
        "\n",
        "Si la différence entre la valeur calculée et la valeur de `energy_100g` est inférieure à 5%, nous pourrons remplacer les valeurs vide du tableau nutritionnel de l'individu par 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n2D6UYQXyvy"
      },
      "source": [
        "# Doubt : how did we come to this formula in the whole cell?\n",
        "# On calcule l'énergie au 100g à partir des valeurs du tableau nutritionnel\n",
        "df_clean[\"energy_100g_calc\"] = df_clean[\"fat_100g\"].fillna(0) * 37 + \\\n",
        "    df_clean[\"proteins_100g\"].fillna(0) * 17 + \\\n",
        "    df_clean[\"carbohydrates_100g\"].fillna(0) * 17 + \\\n",
        "    df_clean[\"fiber_100g\"].fillna(0) * 8\n",
        "\n",
        "# On calcule la distance normalisée entre les 2 énergies\n",
        "df_clean[\"energy_100g_diff\"] = (df_clean[\"energy_100g\"] - df_clean[\"energy_100g_calc\"]).abs()\n",
        "df_clean[\"energy_100g_diff\"] = df_clean[\"energy_100g_diff\"] / (df_clean[\"energy_100g\"] + 1e-6)\n",
        "\n",
        "cols = [\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "]\n",
        "\n",
        "# Si différence des énergie est inférieure à 5% alors on remplace les valeurs manquantes par 0\n",
        "df_clean.loc[df_clean[\"energy_100g_diff\"] < 0.05, cols].fillna(0, inplace=True)\n",
        "\n",
        "# On supprime les colonnes temporaires\n",
        "df_clean = df_clean.drop(columns=[\"energy_100g_calc\", \"energy_100g_diff\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saiXKzksXyv3"
      },
      "source": [
        "Enfin, nous allons supprimer les tableau nutritionnels dont toutes les valeurs sont à 0 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr6SSZQzXyv6"
      },
      "source": [
        "# Doubt : Why the df is filter for energy valu >= 8 ?\n",
        "all_cols = [\n",
        "    \"energy_100g\",\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "]\n",
        "\n",
        "cols = [\n",
        "    \"fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "]\n",
        "\n",
        "# On sélectionne les produits ayant une énergie non nulle \n",
        "# afin de ne pas prendre en compte les eaux minérales par exemple.\n",
        "mask = df_clean[\"energy_100g\"] >= 8\n",
        "# Parmis ces produits, on sélectionne ceux qui ont toutes leurs valeurs à 0\n",
        "mask &= (df_clean[cols] == 0).all(axis=1)\n",
        "\n",
        "# On supprime ces valeurs aberrantes\n",
        "df_clean.loc[mask, all_cols] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Doo4MKUGXyv-"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "tmp = msno.bar(df_clean, ax=ax)\n",
        "ax.set_title(\n",
        "    \"\"\"\n",
        "    Représentation des valeurs présentes par variable.\n",
        "    \"\"\",\n",
        "    fontsize=16)\n",
        "fig.axes[0].set_ylabel(\"Fréquence de valeurs présentes\", fontsize=16)\n",
        "fig.axes[1].set_ylabel(\"Nombre de valeurs présentes\", fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "F13R-y2kXywE"
      },
      "source": [
        "print_md_notna_sizes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFK_zQ31XywL"
      },
      "source": [
        "On constate que l'on a beaucoup plus de valeurs manquantes dans les valeurs nutritionnelles. Cela vient du fait que l'on a supprimé beaucoup de valeurs aberrantes. Ceci a été réalisé en utilisant des données métier. En effet, la répartition des nutriments peut être très variable d'un produit à l'autre même si ils ont la même appellation. On a donc préféré supprimer toutes données suspectes afin de pouvoir justement mettre en avant les différences entre les produits dans cette analyse.\n",
        "\n",
        "On remarque cependant que l'on a plus d'individus ayant leur tableau nutritionnel complet ! On a pu reconstituer le tableau nutrionnel de **17%** des individus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9h9epRXXywO"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "Les variables `product_name`, `ingredients_text`, `additives_fr` et `allergens` représentent respectivement les noms des produits, les listes d'ingrédients, les additifs et les produits allergènes. Ces données sont des listes de textes et ne respectent pas de format particulier. Le wiki du projet Open Food Facts explique qu'il est effectivement difficile de nettoyer ces données ([en savoir plus](https://wiki.openfoodfacts.org/Ingredients_Extraction_and_Analysis)).\n",
        "\n",
        "Nous allons, à partir de ces variables, créer les variables suivantes qui seront plus facilement exploitables :\n",
        "- `ingredients_n` : nombre d'ingrédients.\n",
        "- `allergens_n` : nombre d'allergènes.\n",
        "- `additifs_n_risque_faible` : nombre d'additifs ayant un risque de sur-exposition très faible ou nul.\n",
        "- `additifs_n_risque_moyen` : nombre d'additifs ayant un risque modéré de sur-exposition.\n",
        "- `additifs_n_risque_eleve` : nombre d'additifs ayant un risque élevé de sur-exposition.\n",
        "- `pn_beverages` : lien entre le nom du produit et la catégorie des boissons.\n",
        "- `pn_cereals_and_potatoes` : lien entre le nom du produit et la catégorie des céréales et pommes de terre.\n",
        "- `pn_composite_foods` : lien entre le nom du produit et la catégorie des produits composés.\n",
        "- `pn_fat_and_sauces` : lien entre le nom du produit et la catégorie des sauces et matières grasses.\n",
        "- `pn_fish_meat_eggs` : lien entre le nom du produit et la catégorie des poissons, des viandes et des oeufs.\n",
        "- `pn_fruits_and_vegetables` : lien entre le nom du produit et la catégorie des fruits et légumes.\n",
        "- `pn_milk_and_dairy_products` : lien entre le nom du produit et la catégorie des produits laitiers.\n",
        "- `pn_salty_snacks` : lien entre le nom du produit et la catégorie des snacks salés.\n",
        "- `pn_sugary_snacks` : lien entre le nom du produit et la catégorie des snacks sucrés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5N881nvXywS"
      },
      "source": [
        "### Création de la variable `ingredients_n`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsmxrcrfXywU"
      },
      "source": [
        "# On convertit le texte représentant les ingrédients en une liste d'ingrédients\n",
        "df_clean[\"ingredient\"] = df_clean[\"ingredients_text\"].transform(lambda x: x.str.split(\",\"))\n",
        "\n",
        "# On crée la nouvelle variable\n",
        "df_clean[\"ingredients_n\"] = df_clean[\"ingredient\"].apply(lambda x: len(x) if isinstance(x, list) else x)\n",
        "\n",
        "# Doubt: why transform done below ?\n",
        "# On explose cette liste en créant un nouveau DataFrame avec une ligne par ingrédient\n",
        "tmp = df_clean[[\"code\", \"ingredient\"]].explode(\"ingredient\").transform(lambda x: x.str.strip()).dropna()\n",
        "\n",
        "# On supprime la variable créée temporairement\n",
        "df_clean = df_clean.drop(columns=[\"ingredient\"])\n",
        "\n",
        "# On nettoie un peu la liste (suppression du padding, texte en minuscule)\n",
        "tmp[\"ingredient\"] = tmp[\"ingredient\"].transform(lambda x: x.str.strip().str.lower())\n",
        "\n",
        "# On sauvegarde le DataFrame qui pourra nous servir pour la suite de l'analyse\n",
        "tmp.to_csv(\"df_clean_ingredients.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD63ay_BXywY"
      },
      "source": [
        "print_md(\"Voici des exemples de la nouvelle variable :\")\n",
        "\n",
        "df_clean[[\"ingredients_text\", \"ingredients_n\"]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrQARg3Xywc"
      },
      "source": [
        "### Création de la variable `allergens_n`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGyYO07fXywg"
      },
      "source": [
        "# On convertit le texte représentant les allergènes en une liste d'allergènes\n",
        "df_clean[\"allergen\"] = df_clean[\"allergens\"].transform(lambda x: x.str.split(\",\"))\n",
        "\n",
        "# On crée la nouvelle variable\n",
        "df_clean[\"allergens_n\"] = df_clean[\"allergen\"].apply(lambda x: len(x) if isinstance(x, list) else x)\n",
        "\n",
        "# La variable \"allergens\" ne contient pas de modalités pour indiquer qu'il n'y a pas\n",
        "# d'allergène. Nous allons donc ajouter la valeur 0 pour les individus qui ont leur\n",
        "# liste d'ingrédient et une valeur manquante pour \"allergens\"\n",
        "mask = df_clean[\"ingredients_text\"].notna() & df_clean[\"allergens\"].isna()\n",
        "df_clean.loc[mask, \"allergens_n\"] = 0\n",
        "\n",
        "# On explose cette liste en créant un nouveau DataFrame avec une ligne par allergène\n",
        "tmp = df_clean[[\"code\", \"allergen\"]].explode(\"allergen\").transform(lambda x: x.str.strip()).dropna()\n",
        "\n",
        "# On supprime la variable créée temporairement\n",
        "df_clean = df_clean.drop(columns=[\"allergen\"])\n",
        "\n",
        "# On nettoie un peu la liste (suppression du padding, texte en minuscule)\n",
        "tmp[\"allergen\"] = tmp[\"allergen\"].transform(lambda x: x.str.strip().str.lower())\n",
        "\n",
        "# On sauvegarde le DataFrame qui pourra nous servir pour la suite de l'analyse\n",
        "tmp.to_csv(\"df_clean_allergens.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eOWElnXXywk"
      },
      "source": [
        "print_md(\"Voici des exemples de la nouvelle variable :\")\n",
        "\n",
        "df_clean.dropna(subset=[\"allergens\"])[[\"allergens\", \"allergens_n\"]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4m3ODk8Xywo"
      },
      "source": [
        "### Création des variables `additifs_risque_*`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av22eqKKXywr"
      },
      "source": [
        "Pour créer ces variables, nous allons utiliser la liste des additifs disponibles sur Open Food Facts ([en savoir plus](https://world.openfoodfacts.org/additives))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3yTJGrnXywt"
      },
      "source": [
        "additifs = pd.read_csv(\"additives.csv\", sep=\",\")\n",
        "additifs.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXmjJZYZXywx"
      },
      "source": [
        "# Commençons par supprimer les variables inutiles\n",
        "additifs = additifs.drop(columns=[\"Code\",\"Products\"])\n",
        "\n",
        "# On ajoute une ligne avec le code de l'additif\n",
        "additifs[\"Additive_code\"] = additifs[\"Additive\"].apply(lambda x: x.split(\" - \")[0])\n",
        "additifs[\"Additive_code\"] = additifs[\"Additive_code\"].transform(lambda x: x.str.strip().str.lower())\n",
        "\n",
        "additifs.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1ZO0l0CXyw0"
      },
      "source": [
        "# On convertit le texte représentant les additifs en une liste d'additifs\n",
        "df_clean[\"additif\"] = df_clean[\"additives_fr\"].transform(lambda x: x.str.split(\",\"))\n",
        "\n",
        "# On explose cette liste en créant un nouveau DataFrame avec une ligne par additif\n",
        "tmp = df_clean[[\"code\", \"additif\"]].explode(\"additif\").transform(lambda x: x.str.strip()).dropna()\n",
        "\n",
        "# On supprime la variable créée temporairement\n",
        "df_clean = df_clean.drop(columns=[\"additif\"])\n",
        "\n",
        "# Doubt : it was not clear how the below cleaning was spotted ?\n",
        "# On nettoie un peu la liste (suppression du padding, suppression de l'entête \"en:\")\n",
        "tmp[\"additif\"] = tmp[\"additif\"].transform(lambda x: x.str.strip().str.replace(\"^en:\", \"\"))\n",
        "\n",
        "# On ajoute une ligne avec le code de l'additif\n",
        "tmp[\"additif_code\"] = tmp[\"additif\"].apply(lambda x: x.split(\" - \")[0])\n",
        "tmp[\"additif_code\"] = tmp[\"additif_code\"].transform(lambda x: x.str.strip().str.lower())\n",
        "\n",
        "# On merge les 2 DataFrames\n",
        "tmp = tmp.reset_index().merge(\n",
        "    additifs[[\"Additive_code\", \"Risk\"]],\n",
        "    how=\"left\",\n",
        "    left_on=\"additif_code\",\n",
        "    right_on=\"Additive_code\"\n",
        ").set_index(\"index\")\n",
        "\n",
        "# On remplace les valeurs des riques par quelque chose de plus lisible\n",
        "tmp[\"risque\"] = tmp[\"Risk\"].replace({\n",
        "    \"No or very low risk of over exposure\": \"faible\",\n",
        "    \"Moderate risk of over exposure\": \"moyen\",\n",
        "    \"High risk of over exposure\": \"eleve\",\n",
        "})\n",
        "\n",
        "tmp = tmp[[\n",
        "    \"code\",\n",
        "    \"additif\",\n",
        "    \"additif_code\",\n",
        "    \"risque\"\n",
        "]]\n",
        "\n",
        "# On sauvegarde le DataFrame qui pourra nous servir pour la suite de l'analyse\n",
        "tmp.to_csv(\"df_clean_additifs.csv\")\n",
        "\n",
        "tmp.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVJHXDOdXyw6"
      },
      "source": [
        "# On convertit le risque en nombre\n",
        "tmp[\"additifs_n_risque_faible\"] = (tmp[\"risque\"] == \"faible\").astype(np.uint8)\n",
        "tmp[\"additifs_n_risque_moyen\"] = (tmp[\"risque\"] == \"moyen\").astype(np.uint8)\n",
        "tmp[\"additifs_n_risque_eleve\"] = (tmp[\"risque\"] == \"eleve\").astype(np.uint8)\n",
        "\n",
        "# On aggrège les données par code du produit\n",
        "cols = [\n",
        "    \"code\",\n",
        "    \"additifs_n_risque_faible\",\n",
        "    \"additifs_n_risque_moyen\",\n",
        "    \"additifs_n_risque_eleve\",\n",
        "]\n",
        "\n",
        "tmp = tmp[cols].groupby(\"code\").aggregate(\"sum\").reset_index()\n",
        "\n",
        "# On merge les 2 DataFrames\n",
        "df_clean = df_clean.reset_index().merge(\n",
        "    tmp,\n",
        "    how=\"left\",\n",
        "    on=\"code\"\n",
        ").set_index(\"index\")\n",
        "\n",
        "# Nous allons remplacer les valeurs manquantes par 0\n",
        "df_clean = df_clean.fillna({\n",
        "    \"additifs_n_risque_faible\": 0,\n",
        "    \"additifs_n_risque_moyen\": 0,\n",
        "    \"additifs_n_risque_eleve\": 0,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4FniDVsXyw-"
      },
      "source": [
        "print_md(\"Voici des exemples des nouvelles variables :\")\n",
        "\n",
        "cols = [\n",
        "    \"additives_fr\",\n",
        "    \"additifs_n_risque_faible\",\n",
        "    \"additifs_n_risque_moyen\",\n",
        "    \"additifs_n_risque_eleve\",\n",
        "]\n",
        "\n",
        "df_clean.dropna(subset=[\"additives_fr\"])[cols].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5iUOYJGXyxC"
      },
      "source": [
        "### Création des variables `pn_*`\n",
        "\n",
        "Nous allons utiliser le principe du target encoding afin de créer des variables numériques à partir de la variable de type catégorie `product_name`. Notre variable cible sera la catégorie du produit `pnns_groups_1`.\n",
        "\n",
        "Pour chaques mots du nom d'un produits, nous allons leurs attribuer un score correspondant à la fréqeunce d'apparition du mot pour une des modalités de la varaible cible. Enfin, nous allons faire la somme des scores de chacun des mots du nom du produit pour chaque modalité de la variable cible.\n",
        "\n",
        "Cela va nous permettre de projecter le nom d'un produit dans un espace où les dimensions représentent les catégories des produits. On obtiendra alors des variables de type numérique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhK2oJVvk5zD"
      },
      "source": [
        "df_clean[\"product_name\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmCiT1AxXyxF"
      },
      "source": [
        "# TODO ANalysie the whole cell\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# On crée un jeu de donnés pour entrainer les encodeurs\n",
        "tmp = df_clean.dropna(subset=[\"pnns_groups_1\"])\n",
        "# On encode la variable cible de type catégorie\n",
        "dummies = pd.get_dummies(tmp[\"pnns_groups_1\"], prefix=\"pn\")\n",
        "dummies.columns = dummies.columns.str.lower().str.replace(\" \", \"_\")\n",
        "\n",
        "# On crée un jeu d'entrainement pour les encodeurs\n",
        "tmp = tmp[\"product_name\"].str.lower().str.replace(\"[^a-zA-Z ]\", \"\").str.strip().str.split()\n",
        "tmp = pd.concat([tmp, dummies], axis=1)\n",
        "tmp = tmp.explode(\"product_name\")\n",
        "\n",
        "# On crée la liste des stop-words dans les prinicpales langues\n",
        "words_to_del = []\n",
        "for l in [\"english\", \"french\", \"dutch\", \"spanish\", \"italian\"]:\n",
        "    words_to_del += stopwords.words(l)\n",
        "\n",
        "# On supprime les stop-words\n",
        "tmp = tmp[~tmp[\"product_name\"].isin(words_to_del)]\n",
        "\n",
        "# On crée la liste des encodeurs (un par modalité de la variable cible)\n",
        "map_encoders = []\n",
        "\n",
        "# Nous utiliseront de l'additive smoothing afin d'éviter l'over-fitting.\n",
        "# m représentent le nombre d'individus minimums qu'il faut pour compenser\n",
        "# l'additive smoothing ajouté par la moyenne générale.\n",
        "m = 0\n",
        "\n",
        "# On entraine les encodeurs (un par modalité de la variable cible)\n",
        "for c in dummies.columns:\n",
        "    # Calcul de la moyenne générale\n",
        "    mean = tmp[c].mean()\n",
        "\n",
        "    # Calcul du nombre d'individus et de la moyenne pour chaque groupe\n",
        "    agg = tmp.groupby(\"product_name\")[c].agg([\"count\", \"mean\"])\n",
        "    counts = agg[\"count\"]\n",
        "    means = agg[\"mean\"]\n",
        "\n",
        "    # Calcule de la moyenne lissée\n",
        "    smooth = (counts * means + m * mean) / (counts + m)\n",
        "\n",
        "    map_encoders.append(smooth)\n",
        "\n",
        "map_encoders = pd.concat(map_encoders, axis=1)\n",
        "\n",
        "tmp = df_clean[\"product_name\"].str.lower().str.replace(\"[^a-zA-Z ]\", \"\").str.strip().str.split(expand=True)\n",
        "\n",
        "# On encode les mots et on additionne le résultat pour chaque modalité de \"pnns_groups_1\"\n",
        "for i in range(map_encoders.shape[1]):\n",
        "    tmp2 = tmp.copy()\n",
        "    for c in tmp2.columns:\n",
        "        tmp2[c] = tmp2[c].map(map_encoders[i])\n",
        "    df_clean[dummies.columns[i]] = tmp2.sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYFM1lIKXyxK"
      },
      "source": [
        "print_md(\"Voici des exemples des nouvelles variables :\")\n",
        "\n",
        "cols = [\n",
        "    \"product_name\",\n",
        "    \"pnns_groups_1\",\n",
        "    \"pn_beverages\",\n",
        "    \"pn_cereals_and_potatoes\",\n",
        "    \"pn_composite_foods\",\n",
        "    \"pn_fat_and_sauces\",\n",
        "    \"pn_fish_meat_eggs\",\n",
        "    \"pn_fruits_and_vegetables\",\n",
        "    \"pn_milk_and_dairy_products\",\n",
        "    \"pn_salty_snacks\",\n",
        "    \"pn_sugary_snacks\",\n",
        "]\n",
        "\n",
        "df_clean.dropna(subset=[\"pnns_groups_1\"])[cols].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QPUX3DKXyxO"
      },
      "source": [
        "## Types des variables\n",
        "\n",
        "Certaines variables représentent un horodatage. Nous allons donc les convertir en type DateTime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzPY0jSFXyxR"
      },
      "source": [
        "# Conversion des variables de type datetime\n",
        "c_t = [c for c in df_clean.columns if c.endswith(\"_t\")]\n",
        "    \n",
        "for c in c_t: \n",
        "    df_clean.loc[:, [c]] = pd.to_datetime(df_clean[c], unit='s', errors=\"coerce\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyc5UjrtXyxW"
      },
      "source": [
        "Voyons les types de nos variables :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9dHHoviEXyxb"
      },
      "source": [
        "df_clean.info(verbose=True, null_counts=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPOsX5NLXyxf"
      },
      "source": [
        "# Liste des variables de type numérique\n",
        "num_cols = df_clean.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Liste des variables de type catégorie\n",
        "cat_cols = df_clean.select_dtypes(include=\"object\").columns.tolist()\n",
        "\n",
        "text = \"Nombre de variables de type :\\n\"\n",
        "text += f\"- **Numérique** : {len(num_cols)}\\n\"\n",
        "text += f\"- **Catégorie** : {len(cat_cols)}\\n\"\n",
        "\n",
        "print_md(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9I3o3ckXyxj"
      },
      "source": [
        "text = \"Voici la liste des variables de type numérique :\\n\"\n",
        "\n",
        "for c in num_cols:\n",
        "    text += f\"- `{c}`\\n\"\n",
        "\n",
        "print_md(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LYidA0IwXyxm"
      },
      "source": [
        "text = \"Voici la liste des variables de type catégorie :\\n\"\n",
        "\n",
        "for c in cat_cols:\n",
        "    text += f\"- `{c}`\\n\"\n",
        "\n",
        "print_md(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-8bPg9XXyxs"
      },
      "source": [
        "## Sauvegarde du jeu de données nettoyé"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgiqMJgaXyxv"
      },
      "source": [
        "# On met les variables dans l'ordre\n",
        "cols = [\n",
        "    \"code\",\n",
        "\n",
        "    \"creator\",\n",
        "    \"last_modified_t\",\n",
        "\n",
        "    \"product_name\",\n",
        "    \"brands\",\n",
        "    \"countries_fr\",\n",
        "    \"pnns_groups_1\",\n",
        "    \"pnns_groups_2\",\n",
        "\n",
        "    \"nutriscore_grade\",\n",
        "    \"nutriscore_score\",\n",
        "\n",
        "    \"ingredients_text\",\n",
        "    \"ingredients_n\",\n",
        "    \n",
        "    \"additives_fr\",\n",
        "    \"additives_n\",\n",
        "    \"additifs_n_risque_faible\",\n",
        "    \"additifs_n_risque_moyen\",\n",
        "    \"additifs_n_risque_eleve\",\n",
        "    \n",
        "    \"allergens\",\n",
        "    \"allergens_n\",\n",
        "\n",
        "    \"energy_100g\",\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "    \n",
        "    \"pn_beverages\",\n",
        "    \"pn_cereals_and_potatoes\",\n",
        "    \"pn_composite_foods\",\n",
        "    \"pn_fat_and_sauces\",\n",
        "    \"pn_fish_meat_eggs\",\n",
        "    \"pn_fruits_and_vegetables\",\n",
        "    \"pn_milk_and_dairy_products\",\n",
        "    \"pn_salty_snacks\",\n",
        "    \"pn_sugary_snacks\"\n",
        "]\n",
        "\n",
        "df_clean = df_clean[cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxdJ8trEXyxz"
      },
      "source": [
        "print_md_df_shape(df_clean, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-zv4KrzXyx4"
      },
      "source": [
        "cols = [\n",
        "    \"product_name\",\n",
        "    \"nutriscore_grade\",\n",
        "    \"ingredients_text\",\n",
        "    \"energy_100g\",\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "]\n",
        "\n",
        "# On supprime les individus ayant des valeurs manquantes dans les variables ci-dessus\n",
        "df_clean = df_clean.dropna(subset=cols)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "msno.bar(df_clean, ax=ax)\n",
        "ax.set_title(\n",
        "    \"\"\"\n",
        "    Représentation des valeurs présentes par variable.\n",
        "    \"\"\",\n",
        "    fontsize=16)\n",
        "fig.axes[0].set_ylabel(\"Fréquence de valeurs présentes\", fontsize=16)\n",
        "fig.axes[1].set_ylabel(\"Nombre de valeurs présentes\", fontsize=16)\n",
        "\n",
        "# plt.savefig('valeurs présentes après nettoyage.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBAF852UXyx9"
      },
      "source": [
        "`brands`, `countries_fr`, `pnns_groups_1` et `pnns_groups_2` contiennent encore des valeurs manquantes.\n",
        "\n",
        "`additives_fr` et `allergens` sont des listes et leurs valeurs manquantes représentent l'abscence d'additifs ou l'absence d'allergènes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9BZZgpAXyyA"
      },
      "source": [
        "# On sauvegarde le jeu de données nettoyé\n",
        "df_clean.to_csv('df_clean.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scmkhP9nXyyD"
      },
      "source": [
        "# Analyse univariée\n",
        "\n",
        "## Chargement du jeu de données nettoyé"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqsrN1vXXyyF"
      },
      "source": [
        "df_clean = pd.read_csv(\"df_clean.csv\",\n",
        "    parse_dates=[\"last_modified_t\"],\n",
        "    low_memory=False\n",
        ")\n",
        "\n",
        "print_md_df_shape(df_clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJvZjAhrXyyL"
      },
      "source": [
        "## Analyse des variables de type numérique\n",
        "\n",
        "### Distribution des variables\n",
        "\n",
        "Commençons par observer les distributions des variables de type numérique :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cByP2lXZXyyO"
      },
      "source": [
        "# On calcule les indicateurs statistiques de base\n",
        "df_stats = df_clean.describe().T\n",
        "\n",
        "# On réorganise et modifie le nom des colonnes\n",
        "df_stats = df_stats[[\n",
        "    \"mean\",\n",
        "    \"std\",\n",
        "    \"min\",\n",
        "    \"max\",\n",
        "    \"25%\",\n",
        "    \"50%\",\n",
        "    \"75%\"\n",
        "]]\n",
        "df_stats.columns = [\n",
        "    \"moyenne\",\n",
        "    \"écart type\",\n",
        "    \"min\",\n",
        "    \"max\",\n",
        "    \"Q1\",\n",
        "    \"Q2\",\n",
        "    \"Q3\"\n",
        "]\n",
        "\n",
        "# On ajoute le nombre de valeurs manquantes\n",
        "na_nb = []\n",
        "for c in df_stats.index:\n",
        "    na_nb.append(df_clean[c].isna().sum())\n",
        "\n",
        "df_stats[\"valeurs manquantes\"] = na_nb\n",
        "\n",
        "# On ajoute le nombre de valeurs nulles\n",
        "zero_nb = []\n",
        "for c in df_stats.index:\n",
        "    zero_nb.append((df_clean[c] == 0.).sum())\n",
        "\n",
        "df_stats[\"valeurs nulles\"] = zero_nb\n",
        "\n",
        "# On ajoute une mesure de l'asymétrie de la distribution\n",
        "df_stats[\"skewness\"] = df_clean.skew()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j6pNwfZojQj"
      },
      "source": [
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Jh3GvV3dXyyR"
      },
      "source": [
        "cols = list(df_stats.index)\n",
        "cols_nb = len(cols)\n",
        "c_nb = 3\n",
        "r_nb = np.ceil(cols_nb / c_nb).astype(int)\n",
        "\n",
        "sz = 16 / c_nb\n",
        "fig = plt.figure(figsize=(sz *  c_nb, sz * r_nb))\n",
        "\n",
        "for i, c in enumerate(cols):\n",
        "    ax = plt.subplot(r_nb, c_nb, i + 1)\n",
        "    ax.set_title(f\"Distribution de {c}\", fontsize=14)\n",
        "    \n",
        "    # Si la courbe est très étalée, on utilise une échelle log\n",
        "    if df_stats.loc[c, \"skewness\"] > 2:\n",
        "        ax.set_yscale(\"log\")\n",
        "        ax.set_ylabel(\"Occurrence (log)\", fontsize=14)\n",
        "    else:\n",
        "        ax.set_ylabel(\"Occurrence\", fontsize=14)\n",
        "\n",
        "    df_clean[c].hist(bins=20, ax=ax)\n",
        "    mean_line = ax.axvline(x=df_clean[c].mean(), linewidth=3, color='g', label=\"moyenne\", alpha=0.7)\n",
        "    med_line = ax.axvline(x=df_clean[c].median(), linewidth=3, color='y', label=\"médiane\", alpha=0.7)\n",
        "    \n",
        "    plt.legend(handles=[mean_line, med_line], loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8isakTuqXyyV"
      },
      "source": [
        "On remarque que seule la variable `nutriscore_score` semble suivre une loi normale multimodale. Ceci semble cohérent car cette variable est correlée à `nutriscore_grade` (voir la suite de l'analyse...) qui est une variable de type catégorie possédant 5 modalités.\n",
        "\n",
        "### Comparaison des indicateurs statistiques\n",
        "\n",
        "- `moyenne` : moyenne arithmétique\n",
        "- `écart type` : écart-type empirique\n",
        "- `min` : valeur minimale\n",
        "- `max` : valeur maximale\n",
        "- `Q1` : 1er quartile (25% des valeurs se trouvent en dessous)\n",
        "- `Q2` : 2ème quartile (50% des valeurs se trouvent en dessous)\n",
        "- `Q3` : 3ème quartile (75% des valeurs se trouvent en dessous)\n",
        "- `valeurs manquantes` : nombre de valeurs manquantes\n",
        "- `valeurs nulles` : nombre de valeurs à 0\n",
        "- `skewness` : skewness empirique indiquant l'asymétrie de la distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkuVAhFxXyyY"
      },
      "source": [
        "# On adapte le nombre de décimals affichées en fonction de la valeur de l'indicateur statistique\n",
        "display(df_stats.style.format(lambda x: f\"{x:.4f}\" if abs(x) < 0.01 else f\"{x:.2f}\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njlHH5SxXyyd"
      },
      "source": [
        "Les produits ont en moyenne 14 ingrédients. Cela peut être compréhensible pour des plats cuisinés mais peut sembler beaucoup pour par exemple une mousse au chocolat.\n",
        "\n",
        "On constate que certains produits ont jusqu'à 9 additifs considérés comme risqués pour la santé.\n",
        "\n",
        "On constate aussi sur le tableau ci-dessus que les produits sont en moyenne principalement composés de glucides et de matières grasses.\n",
        "\n",
        "Enfin, on remarque en observant le skewness que la distribution des valeurs est très élalée à droite sauf pour le `nutriscore_score`. Cela semble logique puisque la plupart des aliments sont composés d'une somme de nutriments. L'étalement à droite représente les produits spécifiques, comme par exemples les huiles, qui seront composés en majorité d'un seul nutriment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od95ZH4EXyyg"
      },
      "source": [
        "### Répartition du nombre d'ingrédients par catégorie de produit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6Yfa7nKXyyj"
      },
      "source": [
        "num_categ_box_chart(\n",
        "    df_clean,\n",
        "    \"ingredients_n\",\n",
        "    \"pnns_groups_1\",\n",
        "    title=\"Répartition des valeurs de ingredients_n en fonction de pnns_groups_1\",\n",
        "    xlabel=\"Catégorie de produit\",\n",
        "    ylabel=\"Nombre d'ingrédients\",\n",
        "    showfliers=False,\n",
        "    rotation=30\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l-SAdy-Xyym"
      },
      "source": [
        "Sans surprise, on constate que les aliments composites (plats cuisinés etc...) sont ceux qui contiennent le plus d'ingrédients et les fuits et les légumes sont ceux qui en contiennet le moins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIQEZ2SxXyyo"
      },
      "source": [
        "### Produits ayant un nombre élevé d'additifs à risque\n",
        "\n",
        "Voyons quelles sont les produits ayant jusqu'à 9 additifs considérés comme risqué pour la santé :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "X26EU18XXyys"
      },
      "source": [
        "# to debug the issue and nutriscore_grade column no data\n",
        "cols = [\n",
        "    \"code\",\n",
        "    \n",
        "    \"creator\",\n",
        "    \"last_modified_t\",\n",
        "    \"product_name\",\n",
        "    \"brands\",\n",
        "    \"pnns_groups_1\",\n",
        "\n",
        "    \"nutriscore_grade\",\n",
        "\n",
        "    \"additives_fr\",\n",
        "    \"additifs_n_risque_eleve\",\n",
        "]\n",
        "\n",
        "df_clean[df_clean[\"additifs_n_risque_eleve\"] >= 8][cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L82Hny1mXyyx"
      },
      "source": [
        "Ces produits sont des snacks sucrés vendus aux US. Il est intéressant de constater qu'ils ont été mis à jour en 2020 et sont donc certainement encore vendu en magasin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK_1t4Z7Xyyz"
      },
      "source": [
        "### Comparaison des distribution des nutriments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "UFzXRxEFXyy2"
      },
      "source": [
        "cols = [\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 10))\n",
        "\n",
        "sns.boxplot(\n",
        "    data=df_clean[cols],\n",
        "    showfliers=False,\n",
        "    showmeans=True,\n",
        "    ax=ax\n",
        ")\n",
        "\n",
        "ax.set_title(\"Comparaison des distributions des nutriments\")\n",
        "ax.set_ylabel(\"Quantité en g\")\n",
        "\n",
        "# plt.savefig('comparaison des distributions des nutriments.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJCTYC5SXyy5"
      },
      "source": [
        "On peut voir visuellement ce que l'on avait constaté dans le tableau des indicateurs statistiques (il y a en moyenne plus de glucides et de matières grasses dans nos aliments).\n",
        "\n",
        "Il est aussi intéressant de constater les valeurs des quartiles Q3 indiquant que 75% des aliments sont en dessous de ces seuil :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TJWeiAJ0Xyy9"
      },
      "source": [
        "text = \"\"\n",
        "\n",
        "for i, v in df_clean[cols].quantile(0.75).items():\n",
        "    text += f\"- `{i}` : {v:0.2f}g\\n\"\n",
        "    \n",
        "print_md(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjQxR4J4XyzB"
      },
      "source": [
        "## Analyse des variables de type catégorie\n",
        "\n",
        "### Comparaison des indicateurs statistiques\n",
        "\n",
        "- `nombre de valeurs` : nombre de valeurs présentes\n",
        "- `nombre de modalités` : nombre de modalités de la variable\n",
        "- `mode` : modalité qui apparait le plus souvent\n",
        "- `effectif du mode` : nombre de fois où le mode apparait\n",
        "- `fréquence du mode` : fréquence d'apparition du mode\n",
        "- `valeurs manquantes` : nombre de valeurs manquantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhtvJVdhLsCq"
      },
      "source": [
        "df_clean.describe(include=['object']).T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SYglfX-XyzE"
      },
      "source": [
        "df_stats = df_clean[df_clean.columns.difference([\"code\"])].describe(include=['object']).T\n",
        "\n",
        "df_stats.columns = [\n",
        "    \"nombre de valeurs\",\n",
        "    \"nombre de modalités\",\n",
        "    \"mode\",\n",
        "    \"effectif du mode\",\n",
        "]\n",
        "\n",
        "df_stats[\"fréquence du mode\"] = df_stats[\"effectif du mode\"] / df_stats[\"nombre de valeurs\"]\n",
        "\n",
        "# On ajoute le nombre de valeurs manquantes\n",
        "inconnu_nb = []\n",
        "for c in df_stats.index:\n",
        "    inconnu_nb.append(df_clean[c].isna().sum())\n",
        "\n",
        "df_stats[\"valeurs manquantes\"] = inconnu_nb\n",
        "\n",
        "display(df_stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mF4xDdvXyzJ"
      },
      "source": [
        "Excepté `nutriscore_grade`, `pnns_groups_1` et `pnns_groups_2`, les autres variables ont toutes un grand nombre de modalités. Ceci est du au fait que ce sont des variables de type texte. Elles ne respectent pas de format spécifique et sont donc très irrégulières. Par exemple, pour représenter la valeur `Royaume-Uni`, on pourrait avoir dans nos données :\n",
        "- Royaume-Uni\n",
        "- royaume uni,france\n",
        "- UK\n",
        "- etc...\n",
        "\n",
        "On rappelle aussi que certaines variables, comme `ingredients_text`, sont au format csv et représentent des listes de textes. Cela explique aussi pourquoi il y a de si grands nombres de modalités."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T75ManDcXyzM"
      },
      "source": [
        "### Distribution de la variable `creator`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRkfxmA0XyzP"
      },
      "source": [
        "# Doubt: cannot solve the issue\n",
        "categ_plot_pie_chart(df_clean, \"creator\", 4)\n",
        "# categ_plot_bar_chart(df_clean, \"creator\", 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NpcUgjl6mmj"
      },
      "source": [
        "categ_plot_bar_chart(df_clean, \"creator\", 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WtbXpB0XyzU"
      },
      "source": [
        "57% des produits proviendraient du département de l'agriculture des US.\n",
        "\n",
        "On a ensuite 16% des produits qui ont été rentré par `kiliweb` sur qui nous n'avont pas beaucoup d'information.\n",
        "\n",
        "On retrouve ensuite des contributeurs anonymes avec 8% des produits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltAKcfTPXyzY"
      },
      "source": [
        "### Distribution de la variable `product_name`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jYCh6HNXyza"
      },
      "source": [
        "get_cat_var_emp_dist_df(df_clean, \"product_name\", k=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCSEbGpMXyzf"
      },
      "source": [
        "Les noms qui apparaissent le plus souvent sont `Ice cream` et `Potato chips`. Les noms sont en anglais, il s'agit donc probablement de produits vendus aux US. Il est cependant vrai que, même en France, le rayons des chips comporte de nombreuses variétés et marques différentes.\n",
        "\n",
        "La fréquence des effectifs reste très faible. En effet, le nom de chaque produit peut varier en fonction des ses particularités, de son pays de vente etc... Il faudrait donc nettoyer ces valeurs et les uniformiser afin de pouvoir les exploiter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRaXbxG2Xyzi"
      },
      "source": [
        "### Distribution de la variable `countries_fr`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "9g_zWGo4Xyzl"
      },
      "source": [
        "categ_plot_pie_chart(df_clean, \"countries_fr\", 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6IYr0Km6xQb"
      },
      "source": [
        "categ_plot_bar_chart(df_clean, \"countries_fr\", 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR0KyIGCXyzp"
      },
      "source": [
        "59% des produits de notre jeu de données nettoyé sont vendus aux US et 23% en France.\n",
        "\n",
        "Dans le jeu de données inital (avant nettoyage) ces chiffres étaient presques inversés. Il y a donc beaucoup de produits vendus en France qui ont des valeurs manquantes/aberrantes et qui ont été supprimés durant le nettoyage des données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi4cYkVAXyzs"
      },
      "source": [
        "### Distribution de la variable `brands`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BaEv8PWGXyzu"
      },
      "source": [
        "tmp = df_clean[[\"brands\"]].fillna(\"Unknown\")\n",
        "\n",
        "get_cat_var_emp_dist_df(tmp, \"brands\", k=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZBrpZI0Xyzy"
      },
      "source": [
        "Les 5 marques les plus représentées sont française. Cependant, la variable `brands` contient 28% de valeurs manquante. Voyons dans quel pays sont vendus les produits n'ayant pas leur marque :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQgeYPRyXyz_"
      },
      "source": [
        "pd.DataFrame(df_clean[df_clean[\"brands\"].isna()][\"countries_fr\"].value_counts()[:5]).rename(\n",
        "    columns={\"countries_fr\": \"Nombre de produits sans marque\"}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0vXTYVzXy07"
      },
      "source": [
        "On constate que la grande majorité des produits n'ayant pas de valeur dans la variable `brands` sont vendus aux US."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja4LI2FTXy0-"
      },
      "source": [
        "### Distribution de la variable `pnns_groups_1`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4H4duKUPXy1A"
      },
      "source": [
        "tmp = df_clean[[\"pnns_groups_1\"]].fillna(\"Unknown\")\n",
        "\n",
        "categ_plot_pie_chart(tmp, \"pnns_groups_1\", 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER44V92d7abH"
      },
      "source": [
        "categ_plot_bar_chart(tmp, \"pnns_groups_1\", 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjdtvoDZXy1F"
      },
      "source": [
        "17% des produits sont des snacks sucrés.\n",
        "\n",
        "Il est à noter que la catégorie la moins représentée est composée de 11000 produits. Il s'agit d'un nombre important de produits et on va donc considérer que chaque catégorie est représentative de sa population."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xgUHXu_Xy1H"
      },
      "source": [
        "### Distribution de la variable `pnns_groups_2`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "h_XbQO0NXy1K"
      },
      "source": [
        "tmp = df_clean[[\"pnns_groups_2\"]].fillna(\"Unknown\")\n",
        "\n",
        "categ_plot_pie_chart(tmp, \"pnns_groups_2\", 15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15aQy7zG7pqQ"
      },
      "source": [
        "\n",
        "categ_plot_bar_chart(tmp, \"pnns_groups_2\", 40, log=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmB0yVwAXy1P"
      },
      "source": [
        "Comme précédemment, on retrouve en tête du classement les biscuits et les gateaux avec 9% des produits et les sucreries avec 7% des produits.\n",
        "\n",
        "On remaque cependant que le nombre de produits par catégorie décroit de façon exponentielle. Les dernières catégories contiennent moins de 1000 individus contre 33000 pour la première catégorie.\n",
        "\n",
        "Nous allons privilégier `pnns_groups_1` pour le reste de cette analyse car elle a une répartition plus uniforme des individus par catégorie de produit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_Mng8NIXy1S"
      },
      "source": [
        "### Distribution de la variable `nutriscore_grade`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lts7WYkXy1V"
      },
      "source": [
        "categ_plot_pie_chart(df_clean, \"nutriscore_grade\", 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c-kPoTP76rg"
      },
      "source": [
        "categ_plot_bar_chart(df_clean, \"nutriscore_grade\", 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVyRQO_cXy1a"
      },
      "source": [
        "Cette variable représente le Nutri-Score (code couleur avec les lettre A, B, C, D ou E) visible sur la plupart des embalages des produits. Le grade **d** semble le plus représenté avec 29% des produits.\n",
        "\n",
        "Le grade le moins représenté est le grade **b** mais il est quand même attribué à environ 54000 produits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rSWCzPjXy1d"
      },
      "source": [
        "### Distribution de la variable `ingredients_text`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo8zZRcTXy1f"
      },
      "source": [
        "# On charge le DataFrame créé lors du nettoyage des données et\n",
        "# où on avait explosé la liste des ingrédients.\n",
        "tmp = pd.read_csv(\"df_clean_ingredients.csv\", low_memory=False)\n",
        "tmp.head(2)\n",
        "\n",
        "get_cat_var_emp_dist_df(tmp, \"ingredient\", k=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXWy_L5GXy1k"
      },
      "source": [
        "Les ingrédients les plus utilisés sont le sel, le sucre et l'eau. On remarque que les 6 modalités les plus représentés sont des doublons : les 3 premières sont en anglais et les 3 suivantes sont en français. Ceci illustre bien le fait qu'il y a un gros travail à faire sur les valeurs des ingrédients afin de pouvoir exploiter cette variable. Nous ne l'utiliseront pas pour la suite de cette analyse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGOciyJ4Xy1q"
      },
      "source": [
        "### Distribution de la variable `additives_fr`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfpAMO4rXy1t"
      },
      "source": [
        "# On charge le DataFrame créé lors du nettoyage des données et\n",
        "# où on avait explosé la liste des additifs.\n",
        "tmp = pd.read_csv(\"df_clean_additifs.csv\", low_memory=False)\n",
        "tmp.head(2)\n",
        "\n",
        "categ_plot_pie_chart(tmp, \"additif\", 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay_UKUUF8D2X"
      },
      "source": [
        "categ_plot_bar_chart(tmp, \"additif\", 20, log=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OcOlQJYXy1z"
      },
      "source": [
        "L'additif le plus utilisé est l'acide citrique qui est présent dans 8% des produits ayant des additifs. Ceci n'est pas étonnant puisqu'il s'agit d'un assaisonnement très fréquemment utilisé en cuisine (par exemple via l'utilisation du ju de citron)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_PSwIXnXy12"
      },
      "source": [
        "pct = df_clean[df_clean[\"additives_n\"] > 0].shape[0] * 100 / df_clean.shape[0]\n",
        "\n",
        "print_md(f\"\"\"Il est a noté que les chiffres ci-dessus concernent les produits ayant des additifs, \n",
        "soit {int(pct)}% des produits du jeu de données.\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgrOQ9AJXy16"
      },
      "source": [
        "### Distribution de la variable `allergens`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33bDdjfQXy19"
      },
      "source": [
        "# On charge le DataFrame créé lors du nettoyage des données et\n",
        "# où on avait explosé la liste des allergènes.\n",
        "tmp = pd.read_csv(\"df_clean_allergens.csv\", low_memory=False)\n",
        "tmp.head(2)\n",
        "\n",
        "get_cat_var_emp_dist_df(tmp, \"allergen\", k=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usZ8OyPSXy2A"
      },
      "source": [
        "29% des allergène sont de type produit laitier. En retrouve ensuite le gluten avec 24% et le soja avec 10% des allergènes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBy-WU9EXy2D"
      },
      "source": [
        "pct = df_clean[df_clean[\"allergens_n\"] > 0].shape[0] * 100 / df_clean.shape[0]\n",
        "\n",
        "print_md(f\"\"\"Il est a noté que les chiffres ci-dessus concernent les produits ayant des allergènes, \n",
        "soit {int(pct)}% des produits du jeu de données.\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bprDQRyRXy2I"
      },
      "source": [
        "## Comparaison entre la France et les US\n",
        "\n",
        "Les 2 pays les plus représentés sont la France et les US. Continuons notre analyse en observant les différences des distributions entre ces 2 pays.\n",
        "\n",
        "### Evolution du nombre de produits ajoutés dans le jeu de données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTtvMV_0Xy2L"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(16, 10))\n",
        "\n",
        "for p in [\"États-Unis\", \"France\"]:\n",
        "    # On récupère les produits vendus dans le pays p\n",
        "    tmp = df_clean[df_clean[\"countries_fr\"] == p][[\"last_modified_t\"]]. \\\n",
        "        sort_values(\"last_modified_t\").copy()\n",
        "    \n",
        "    # On ajoute le nombre de produits présents à chaque date\n",
        "    tmp[p] = 1\n",
        "    tmp[p] = tmp[p].cumsum()\n",
        "    \n",
        "    tmp = tmp.set_index(\"last_modified_t\")\n",
        "    \n",
        "    tmp.plot(ax=ax)\n",
        "\n",
        "ax.set_title(\"Evolutions du nombre de produits ajoutés dans le jeu de données par pays de vente\")\n",
        "ax.set_xlabel(\"Date\")\n",
        "ax.set_ylabel(\"Nombre total de produits\")\n",
        "ax.set_yscale(\"log\")\n",
        "    \n",
        "# plt.savefig('Evolutions du nombre de produits ajoutés dans le jeu de données par pays de vente.png', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oylw-BiGXy2P"
      },
      "source": [
        "Les premiers produits français ont été ajouté en 2012. Depuis, il y a eu des ajout de produits réguliers qui suivent une courbre exponentielle (l'échelle des ordonnées est en log).\n",
        "\n",
        "Les premiers produits des US ont été ajoutés en 2015. On observe des pics d'ajouts de produits. Il s'agit certainement d'upload de données venant d'une autre base de données.\n",
        "\n",
        "Les produits et leurs compositions changent régulièrement. Pour cette partie de l'analyse, nous allons donc comparer les produits ajoutés à partir de 2015. Nous allons aussi prendre au hazard 2000 produits dans chaque catégorie de produits de la variable `pnns_groups_1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VYWdqMJXy2S"
      },
      "source": [
        "# On filtre par date de dernière modification du produit et par pays\n",
        "df_sample = df_clean[df_clean[\"last_modified_t\"].dt.year >= 2015]\n",
        "df_sample = df_sample[df_sample[\"countries_fr\"].isin([\"France\", \"États-Unis\"])]\n",
        "\n",
        "# On va prendre au hazard 2000 produits dans chaque catégorie de produits\n",
        "df_sample = df_sample.groupby(\n",
        "    [\"countries_fr\", \"pnns_groups_1\"],\n",
        "    group_keys=False\n",
        ").apply(lambda x: x.sample(min(len(x), 2000)))\n",
        "\n",
        "outs = []\n",
        "\n",
        "# On affiche les distribution empirique des catégories de produits\n",
        "for p in [\"France\", \"États-Unis\"]:\n",
        "    outs.append(widgets.Output())\n",
        "\n",
        "    with outs[-1]:\n",
        "        tmp = get_cat_var_emp_dist_df(\n",
        "            df_sample[df_sample[\"countries_fr\"] == p],\n",
        "            \"pnns_groups_1\"\n",
        "        )\n",
        "\n",
        "        print_md(f\"<center>Distribution empirique des </br>modalités de pnns_groups_1 dans le pays {p}</center>\")\n",
        "        display(tmp)\n",
        "\n",
        "hbox = widgets.HBox(outs)\n",
        "hbox.layout.justify_content=\"space-around\"\n",
        "\n",
        "display(hbox)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2QHzR7KXy2Y"
      },
      "source": [
        "### Répartition des nutriments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_5ZfthWlXy2a"
      },
      "source": [
        "def num_box_chart(\n",
        "        df,\n",
        "        num_cols,\n",
        "        cat_col,\n",
        "        cat_include,\n",
        "        title=\"\",\n",
        "        xlabel=\"\",\n",
        "        ylabel=\"\",\n",
        "        save=False\n",
        "    ):\n",
        "    # On filtre les données numériques et la catégorie qui nous intéresse\n",
        "    tmp = df[df[cat_col].isin(cat_include)][num_cols + [cat_col]]\n",
        "\n",
        "    # On transforme les colonnes numériques en catégorie\n",
        "    tmp = tmp.melt(\n",
        "        id_vars=cat_col,\n",
        "        var_name=xlabel,\n",
        "        value_name=ylabel\n",
        "    )\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(16, 10))\n",
        "    \n",
        "    meanprops = {\n",
        "        'marker':'o',\n",
        "        'markeredgecolor':'black',\n",
        "        'markerfacecolor':'firebrick'\n",
        "    }\n",
        "\n",
        "    sns.boxplot(\n",
        "        data=tmp,\n",
        "        x=xlabel,\n",
        "        y=ylabel,\n",
        "        hue=cat_col,\n",
        "        showfliers=False,\n",
        "        showmeans=True,\n",
        "        meanprops=meanprops,\n",
        "        ax=ax\n",
        "    )\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"\")\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(f\"{title}.png\", bbox_inches='tight')\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "num_cols = [\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\"\n",
        "]\n",
        "\n",
        "cat_col = \"countries_fr\"\n",
        "cat_include = [\"États-Unis\", \"France\"]\n",
        "\n",
        "num_box_chart(\n",
        "    df_sample,\n",
        "    num_cols=num_cols,\n",
        "    cat_col=cat_col,\n",
        "    cat_include=cat_include,\n",
        "    title=\"Répartition des nutriments par pays\",\n",
        "    xlabel=\"type de nutriment\",\n",
        "    ylabel=\"nutriments en g\",\n",
        "#     save=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7V3D3pRXy2f"
      },
      "source": [
        "On constate que les répartition des proportions entre les différents types de nutriments sont similaires entre les 2 pays.\n",
        "\n",
        "On notera qu'il y a une plus grande proportion de produits aux US qui contiennent plus de glucides qu'en France."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTx3PHtbXy2j"
      },
      "source": [
        "### Répartition du nombre d'ingrédients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "a8TLjh2zXy2l"
      },
      "source": [
        "num_cols = [\n",
        "    \"ingredients_n\",\n",
        "]\n",
        "\n",
        "cat_col = \"countries_fr\"\n",
        "cat_include = [\"États-Unis\", \"France\"]\n",
        "\n",
        "num_box_chart(\n",
        "    df_sample,\n",
        "    num_cols=num_cols,\n",
        "    cat_col=cat_col,\n",
        "    cat_include=cat_include,\n",
        "    title=\"Répartition du nombre d'ingrédients par pays\",\n",
        "    xlabel=\"ingrédients\",\n",
        "    ylabel=\"nombre d'ingrédients\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP4wWRkPXy2q"
      },
      "source": [
        "On constate qu'il y a un peu plus d'ingrédients dans les produits vendus aux US qu'en France :\n",
        "- Aux **US**, 75% des produits ont moins de **19 ingrédients**.\n",
        "- En **France** 75% des produits ont moins de **14 ingrédients**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuo-76jaXy2t"
      },
      "source": [
        "### Répartition du nombre d'additifs et d'allergènes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7gpRVW8ZXy2w"
      },
      "source": [
        "num_cols = [\n",
        "    \"additives_n\",\n",
        "    \"additifs_n_risque_faible\",\n",
        "    \"additifs_n_risque_moyen\",\n",
        "    \"additifs_n_risque_eleve\",\n",
        "    \"allergens_n\",\n",
        "]\n",
        "\n",
        "cat_col = \"countries_fr\"\n",
        "cat_include = [\"États-Unis\", \"France\"]\n",
        "\n",
        "num_box_chart(\n",
        "    df_sample,\n",
        "    num_cols=num_cols,\n",
        "    cat_col=cat_col,\n",
        "    cat_include=cat_include,\n",
        "    title=\"Répartition du nombre d'additifs et d'allergènes par pays\",\n",
        "    xlabel=\"additifs et allergènes\",\n",
        "    ylabel=\"nombre d'additifs ou d'allergènes\",\n",
        "#     save=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ4XB7MHXy20"
      },
      "source": [
        "25% des produits vendus en France ont au moins 1 allergène."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94gS9jfLXy23"
      },
      "source": [
        "# Analyse bivariée"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nptTeupKXy26"
      },
      "source": [
        "## Correlation entre `nutriscore_score` et `nutriscore_grade`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "962ffm2NXy2-"
      },
      "source": [
        "num_categ_box_chart(\n",
        "    df_clean,\n",
        "    \"nutriscore_score\",\n",
        "    \"nutriscore_grade\",\n",
        "    title=\"Répartition des valeurs de nutriscore_score en fonction de nutriscore_grade\",\n",
        "    xlabel=\"Nutri-Score\",\n",
        "    ylabel=\"Score\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EydD-PwEXy3D"
      },
      "source": [
        "Il semble effectivement qu'il y ait une relation linéaire entre `nutriscore_score` et `nutriscore_grade`.\n",
        "\n",
        "Nous allons effectuer une analyse de la variance (ANOVA) afin de quantifier cette corrélation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYvh19IgXy3H"
      },
      "source": [
        "# Doubt : undertstand the cell\n",
        "# Rapport de corrélation\n",
        "def eta_squared(df, x, y):\n",
        "    tmp = df.dropna(subset=[x, y])\n",
        "    x, y = tmp[x], tmp[y]\n",
        "    \n",
        "    moyenne_y = y.mean()\n",
        "    classes = []\n",
        "    for classe in x.unique():\n",
        "        yi_classe = y[x==classe]\n",
        "        classes.append({'ni': len(yi_classe),\n",
        "                        'moyenne_classe': yi_classe.mean()})\n",
        "    # Variation totale\n",
        "    SCT = sum([(yj-moyenne_y)**2 for yj in y])\n",
        "    \n",
        "    # Variation interclasse\n",
        "    SCE = sum([c['ni']*(c['moyenne_classe']-moyenne_y)**2 for c in classes])\n",
        "    \n",
        "    return SCE/SCT\n",
        "    \n",
        "c = eta_squared(df_clean, \"nutriscore_grade\", \"nutriscore_score\")\n",
        "print_md(f\"Le rapport de correlation entre `nutriscore_score` et `nutriscore_grade` est de : **{c:0.2f}**\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7E-Ac5IXy3N"
      },
      "source": [
        "Il y a clairement une forte correlation entre ces 2 variables.\n",
        "\n",
        "On remarque cepdendant la présence d'outliers, notamment pour les nutriscore de grade **a** et **b**. Voyons quelques exemples de ces produits :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el8cWu69Xy3S"
      },
      "source": [
        "df_clean[(df_clean[\"nutriscore_grade\"] == \"a\") & (df_clean[\"nutriscore_score\"] > 4)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7uw_mw0Xy3X"
      },
      "source": [
        "On remarque que la plupart des outliers du grade **a** sont des eaux minérales ([en savoir plus sur le calcul du Nutri-Score](https://www.santepubliquefrance.fr/determinants-de-sante/nutrition-et-activite-physique/articles/nutri-score))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VveGZYpWXy3c"
      },
      "source": [
        "## Analyse des correlations\n",
        "\n",
        "### Correlations entre les variables de type numérique et les variables de type catégorie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wknCITZyXy3h"
      },
      "source": [
        "rows = [\n",
        "    \"nutriscore_grade\",\n",
        "    \"pnns_groups_1\",\n",
        "    \"pnns_groups_2\",\n",
        "]\n",
        "\n",
        "cols = [\n",
        "    \"nutriscore_score\",\n",
        "    \"ingredients_n\",\n",
        "    \"additives_n\",\n",
        "    \"additifs_n_risque_faible\",\n",
        "    \"additifs_n_risque_moyen\",\n",
        "    \"additifs_n_risque_eleve\",\n",
        "    \"allergens_n\",\n",
        "    \"energy_100g\",\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "]\n",
        "\n",
        "data = {}\n",
        "\n",
        "for r in rows:\n",
        "    data[r] = []\n",
        "    for c in cols:\n",
        "        es = eta_squared(df_clean, r, c)\n",
        "        data[r].append(es)\n",
        "\n",
        "tmp = pd.DataFrame.from_dict(data, orient='index', columns=cols)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "sns.heatmap(tmp.T, annot=True, ax=ax)\n",
        "\n",
        "ax.set_title(\"Matrice des corrélations entre les variables de type numérique et 3 variables de type catégorie\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRFJZUiFXy3o"
      },
      "source": [
        "On observe des correlations modérées entre `nutriscore_score`, `energy_100g`, `carbohydrates_100g` et les catégorie de produits. On remarque que plus on a de modalités dans les catégories (`pnns_groups_2`), plus ces correlations sont fortes.\n",
        "\n",
        "Cela semble cohérent car certaines catégories de produits, comme les snacks sucrés, vont avoir une énergie aux 100g très élevée et un nutriscore très mauvais. Alors que des catégories comme les fruits et les légumes vont certainement regrouper des produits ayant une énergie aux 100g faible et un très bon nutriscore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5rJQiqpXy3r"
      },
      "source": [
        "num_categ_box_chart(\n",
        "    df_clean,\n",
        "    \"energy_100g\",\n",
        "    \"pnns_groups_1\",\n",
        "    title=\"Répartition des valeurs de energy_100g en fonction de pnns_groups_1\",\n",
        "    xlabel=\"Catégorie des produits\",\n",
        "    ylabel=\"Energie en kJ\",\n",
        "    showfliers=False,\n",
        "    rotation=30\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzLo8tNvXy3z"
      },
      "source": [
        "On constate effectivement que la majorité des fruits et légumes ont une énergie aux 100g assez basse contrairement aux snacks sucrés."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTPsDDZPXy34"
      },
      "source": [
        "num_categ_box_chart(\n",
        "    df_clean,\n",
        "    \"nutriscore_score\",\n",
        "    \"pnns_groups_1\",\n",
        "    title=\"Répartition des valeurs de nutriscore_score en fonction de pnns_groups_1\",\n",
        "    xlabel=\"Catégorie des produits\",\n",
        "    ylabel=\"Score de nutrition\",\n",
        "    showfliers=False,\n",
        "    rotation=30\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnk7-s75Xy38"
      },
      "source": [
        "On constate de même que le nutriscore de la majorité des fruits et légumes est bas contrairement à celui des snacks sucrés."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W126BOqXy4C"
      },
      "source": [
        "### Correlations entre les variables de type numérique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Q6CcY6kXXy4F"
      },
      "source": [
        "cols = [\n",
        "    \"nutriscore_score\",\n",
        "    \"ingredients_n\",\n",
        "    \"additives_n\",\n",
        "    \"additifs_n_risque_faible\",\n",
        "    \"additifs_n_risque_moyen\",\n",
        "    \"additifs_n_risque_eleve\",\n",
        "    \"allergens_n\",\n",
        "    \"energy_100g\",\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "]\n",
        "\n",
        "corr = df_clean[cols].corr()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 12))\n",
        "\n",
        "sns.heatmap(corr, annot=True, ax=ax)\n",
        "ax.set_title(\"Matrice des correlations des variables de type numérique\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8cDKRNdXy4M"
      },
      "source": [
        "Les coefficients de correlation ci-dessus nous donne les informations suivantes :\n",
        "- Correlation positive élevée entre le nombre d'ingrédients et le nombre d'additifs\n",
        "- Correlation positive élevée entre le nombre d'additifs et le nombre d'additifs ayant un risque élevé pour la santé\n",
        "- Corrélations positives modérées entre le score de nutrition et :\n",
        "    - L'énergie aux 100g\n",
        "    - Les matières grasses\n",
        "    - Les matières grasses saturées\n",
        "    - Les sucres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2NzEJ4YXy4R"
      },
      "source": [
        "### Correlations entre les variables de type catégorie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5TkCTDwXy4X"
      },
      "source": [
        "def corr_cat_cat_heatmap(df, c1, c2, title):\n",
        "    # On supprimes les individus ayant des valeurs manquantes\n",
        "    tmp = df.dropna(subset=[c1, c2])\n",
        "\n",
        "    # On crée le tableau de contingence\n",
        "    cont = tmp[[c1, c2]].pivot_table(\n",
        "        index=c1,\n",
        "        columns=c2,\n",
        "        aggfunc=len,\n",
        "        margins=True,\n",
        "        margins_name=\"Total\",\n",
        "    )\n",
        "    cont\n",
        "\n",
        "    tx = cont.loc[:,[\"Total\"]]\n",
        "    ty = cont.loc[[\"Total\"],:]\n",
        "    n = len(tmp)\n",
        "    indep = tx.dot(ty) / n\n",
        "\n",
        "    c = cont.fillna(0) # On remplace les valeurs nulles par 0\n",
        "    measure = (c - indep) ** 2 / indep\n",
        "    xi_n = measure.sum().sum()\n",
        "    table = measure / xi_n\n",
        "    \n",
        "    h = 0.6 * len(cont)\n",
        "    fig, ax = plt.subplots(figsize=(14, h))\n",
        "    \n",
        "    sns.heatmap(table.iloc[:-1,:-1], annot=c.iloc[:-1,:-1], ax=ax)\n",
        "#     sns.heatmap(table.iloc[:-1,:-1], annot=table.iloc[:-1,:-1], ax=ax)\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "corr_cat_cat_heatmap(\n",
        "    df_clean,\n",
        "    \"pnns_groups_1\",\n",
        "    \"nutriscore_grade\",\n",
        "    \"Matrice des correlations entre les modalités des variables pnns_groups_1 et nutriscore_grade\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGsZVL6LXy4g"
      },
      "source": [
        "Comme nous l'avons supposé, on observe effectivement des correlations entre les catégories des produits et le Nutri-Score. On remarquera notamment :\n",
        "- Corrélation entre la catégorie **céréales et pommes de terre** et le Nutri-Score **a**.\n",
        "- Corrélation entre la catégorie **Fruits et légumes** et le Nutri-Score **a**.\n",
        "- Corrélation entre la catégorie **Snacks sucrés** et le Nutri-Score **e**.\n",
        "\n",
        "Il est aussi intéressant de constater qu'il y a des produits du type **Fruits et légumes** qui ont un grade **d** ! Observons quelques-un des ces produits :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA8BkYv4Xy4t"
      },
      "source": [
        "df_clean[(df_clean[\"pnns_groups_1\"] == \"Fruits and vegetables\") & (df_clean[\"nutriscore_grade\"] == \"d\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3laA6FkXy40"
      },
      "source": [
        "Il sagit de produits riches (comme les oléagineux) mais constitués principalement de fruits et de légumes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpL7ICYsXy43"
      },
      "source": [
        "# Analyse multivariée\n",
        "\n",
        "## Analyse en composantes principales (ACP) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0PpdDxwXy47"
      },
      "source": [
        "cols = [\n",
        "    \"nutriscore_score\",\n",
        "\n",
        "    \"ingredients_n\",\n",
        "    \"additives_n\",\n",
        "    \"additifs_n_risque_faible\",\n",
        "    \"additifs_n_risque_moyen\",\n",
        "    \"additifs_n_risque_eleve\",\n",
        "    \"allergens_n\",\n",
        "\n",
        "    \"energy_100g\",\n",
        "    \"fat_100g\",\n",
        "    \"saturated-fat_100g\",\n",
        "    \"carbohydrates_100g\",\n",
        "    \"sugars_100g\",\n",
        "    \"fiber_100g\",\n",
        "    \"proteins_100g\",\n",
        "    \"salt_100g\",\n",
        "    \n",
        "    \"pn_beverages\",\n",
        "    \"pn_cereals_and_potatoes\",\n",
        "    \"pn_composite_foods\",\n",
        "    \"pn_fat_and_sauces\",\n",
        "    \"pn_fish_meat_eggs\",\n",
        "    \"pn_fruits_and_vegetables\",\n",
        "    \"pn_milk_and_dairy_products\",\n",
        "    \"pn_salty_snacks\",\n",
        "    \"pn_sugary_snacks\",\n",
        "]\n",
        "\n",
        "# On ne conserve que les individus ayant toutes les valeurs des variables ci-dessus\n",
        "acp_df = df_clean.dropna(subset=cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPwlDYW9Xy4_"
      },
      "source": [
        "X = acp_df[cols].values\n",
        "names = acp_df[\"code\"].to_numpy()\n",
        "features = cols\n",
        "\n",
        "# On transforme nos variables en variables centrées réduites\n",
        "std_scale = preprocessing.StandardScaler().fit(X)\n",
        "X_scaled = std_scale.transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIp9_zEWEkU7"
      },
      "source": [
        "acp_df[cols].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CbQrchsDz3P"
      },
      "source": [
        "X_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4l4RKBsFi0E"
      },
      "source": [
        "X_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ePCkLOFefR"
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE6Zvyu9Xy5E"
      },
      "source": [
        "# On réalise une PCA\n",
        "pca = decomposition.PCA(n_components=len(features), random_state=0)\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# On récupère le ratio de l'inertie de chaque axe d'inertie\n",
        "evrs = pca.explained_variance_ratio_\n",
        "\n",
        "# On crée et on affiche le diagramme des éboulis de valeurs propres\n",
        "fig, ax = plt.subplots(figsize=(16,10))\n",
        "\n",
        "ax.set_title(\"Diagramme des éboulis de valeurs propres avec somme cumulée\")\n",
        "ax.set_xlabel(\"Rang de l'axe d'inertie\")\n",
        "ax.set_ylabel(\"Taux d'inertie\")\n",
        "\n",
        "ax.bar(np.arange(len(evrs)) + 1, evrs)\n",
        "l1 = ax.plot(np.arange(len(evrs)) + 1, evrs.cumsum(), c=\"red\", marker='o', label=\"Inertie cumulée\")[0]\n",
        "\n",
        "# Ligne représentant le critère de Kaiser\n",
        "k = 1 / len(features)\n",
        "l2 = ax.axhline(k, color=\"grey\", ls=\"--\", alpha=0.7, label=\"Critère de Kaiser\")\n",
        "\n",
        "# On ajoute le taux d'inertie\n",
        "for i, evr in enumerate(evrs):\n",
        "    if evr > k:\n",
        "        ax.text(i + 0.65, evr + .02, f\"{evr:0.2f}\")\n",
        "        \n",
        "plt.legend(handles=[l1, l2], loc='upper left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw10MhwwXy5N"
      },
      "source": [
        "Les 4 premiers axes d'inertie permettent d'expliquer 42% de la variance totale. Les autres axes ont un taux d'intertie trop faible et qui passe rapidement en dessous du critère de Kaiser.\n",
        "\n",
        "Nous allons continuer cette ACP avec les 4 premiers axes d'inertie F1, F2, F3 et F4. Commençons par observer nos variables projectées sur le 1er plan factoriel (F2, F1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr1oiN_1Xy5R"
      },
      "source": [
        "def pca_plot_var(pcs, x_idx, y_idx, circle=False):\n",
        "    # On affiche le cercle de corrélation de chaque plan factoriel\n",
        "    fig, ax = plt.subplots(figsize=(14, 14))\n",
        "\n",
        "    for j, (x, y) in enumerate(zip(pcs[x_idx, :], pcs[y_idx, :])):\n",
        "        ax.plot([0, x], [0, y], color=\"grey\", alpha=0.7)\n",
        "        plt.text(x, y, cols[j], fontsize=14)\n",
        "\n",
        "    if circle:\n",
        "        ax.add_patch(plt.Circle((0, 0), 1, color=\"grey\", alpha=0.7, fill=False))\n",
        "    \n",
        "    ax.axhline(0, color=\"grey\", ls=\"--\", alpha=0.5)\n",
        "    ax.axvline(0, color=\"grey\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "    ax.set_title(f\"Cercle des corrélations sur le plan factoriel (F{x_idx + 1}, F{y_idx + 1})\")\n",
        "    ax.set_xlabel(f\"F{x_idx + 1} ({evrs[x_idx] * 100: 0.1f}%)\")\n",
        "    ax.set_ylabel(f\"F{y_idx + 1} ({evrs[y_idx] * 100: 0.1f}%)\")\n",
        "    \n",
        "    ax.set_aspect(\"equal\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "pcs = pca.components_\n",
        "X_projected = pca.transform(X_scaled)\n",
        "    \n",
        "pca_plot_var(pcs, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GgjJjThXy5W"
      },
      "source": [
        "On constate que les variables qui contribuent le plus à F1 sont `energy_100g`, `nutriscore_score`. On retrouve ensuite `pn_sugary_snacks`, `saturated-fat_100g` et `fat_100g`. On ramqaure que `pn_fruits_and_vegetables` apporte une faible contribution négative. F1 pourrait donc représenter la notion de richesse énergétique d'un produit en termes de matières grasses et de sucres.\n",
        "\n",
        "F2 augmente avec `additives_n` et diminue avec `fat_100g`, `saturated-fat_100g` et `proteins_100g`. F2 pourrait ainsi représenter les additifs qui feraient diminuer la quantité de matières grasses. F2 pourrait aussi simplement représenter les produits qui se différencie ou non des huiles qui sont des produits riches et ayant peu d'ingrédients.\n",
        "\n",
        "Voyons maintenant la projection des variables initiales sur (F4, F3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdNxsbKsXy5Z"
      },
      "source": [
        "pca_plot_var(pcs, 3, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyTb4cDMXy6G"
      },
      "source": [
        "F3 augmente avec `ingredients_n` et `pn_composite_foods` et diminue avec `sugars_100g`. F3 semble représenter les produits salés ayant de nombreux ingrédients.\n",
        "\n",
        "F4 augmente un peu avec `additifs_n_risque_faible` et `additifs_n_risque_moyen` et diminue avec `pn_cereals_and_potatoes` et `carbohydrates_100g`. F4 pourrait indiquer la proportion d'additifs qui permettrait de remplacer les glucides."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0TcSDouXy6J"
      },
      "source": [
        "tmp = acp_df[cols].copy()\n",
        "for i in range(4):\n",
        "    tmp[f\"F{i + 1}\"] = X_projected[:, i]\n",
        "\n",
        "corrMatrix = tmp.corr()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 14))\n",
        "\n",
        "sns.heatmap(corrMatrix.iloc[:-4, -4:], annot=True, ax=ax)\n",
        "ax.set_title(\"Matrice des corrélation avec les axes d'inertie de l'ACP\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGVo_QgiXy6d"
      },
      "source": [
        "On peut constater sur la matrice des corrélation ci-dessus, ce que l'on avait observé sur les cercles des corrélations.\n",
        "\n",
        "Nos nouvelles variables F1, F2, F3 et F4 sont des combinaisons linéaires des variables initiales. Les interprétations ci-dessus ne sont que des hypothèses faites à partir de l'observation des poids des variables initiales dans le calcul des variables F1, F2, F3 et F4. On pourrait vérifier ces hypothèses en projetant des individus dans nos plans factoriels. Voyons donc quelques produits projetés sur les 3 premiers axes d'inerties F1, F2 et F3 :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1miKyS16Xy6g"
      },
      "source": [
        "X_projected = pca.transform(X_scaled)\n",
        "\n",
        "# On crée un DataFrame avec nos individus et en leurs ajoutant les variables F1, F2 et F3\n",
        "tmp = pd.DataFrame(X_projected[:, :3], columns=[f\"F{i + 1}\" for i in range(3)])\n",
        "tmp[\"nutriscore_grade\"] = acp_df[\"nutriscore_grade\"].values\n",
        "tmp[\"pnns_groups_1\"] = acp_df[\"pnns_groups_1\"].values\n",
        "tmp = tmp.dropna()\n",
        "\n",
        "# On va prendre au hazard 100 produits pour chaque Nutri-Score.\n",
        "# Il serait trop lourd d'afficher tous les individus sur un digramme interractif en 3D.\n",
        "tmp = tmp.groupby(\n",
        "    [\"nutriscore_grade\", \"pnns_groups_1\"],\n",
        "    group_keys=False\n",
        ").apply(lambda x: x.sample(min(len(x), 100)))\n",
        "\n",
        "# On affiche les individus en 3D\n",
        "fig = px.scatter_3d(\n",
        "    tmp,\n",
        "    x=\"F1\",\n",
        "    y=\"F2\",\n",
        "    z=\"F3\",\n",
        "    color=\"pnns_groups_1\",\n",
        "    title=f\"Représentation d'un échantillon des individus projetés sur (F1, F2, F3)\"\n",
        ")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-puyOYZCXy6m"
      },
      "source": [
        "En faisant tourner la figure ci-dessus, on remarque que certains groupes de produits sont facilement discernables comme par exemple :\n",
        "- Les poissons, viandes et oeufs.\n",
        "- Les boissons.\n",
        "- Les produits composés.\n",
        "- Les snacks sucrés.\n",
        "\n",
        "Si on prend par exemple les individus extrêmes sur F1, on obtient :\n",
        "- Un fruit ou légume avec une valeur négative sur F1.\n",
        "- Un snack sucré avec une valeur positive sur F1.\n",
        "\n",
        "Cela semble cohérent au vu de l'interprétation de F1 qui indiquerait la richesse énergétique d'un produit en termes de sucres et de matières grasses.\n",
        "\n",
        "Dans le cadre de ce projet, les nouvelles variables crées grâce aux axes d'intertie pourraient servir de filtre lorsque les agents de Santé publique France voudront explorer le jeu de données.\n",
        "\n",
        "## Recherche de groupes de produits via l'algorithme k-means\n",
        "\n",
        "### Recherche du nombre optimal de clusters\n",
        "\n",
        "Nous allons utiliser la méthode du coude qui permet de trouver le nombre de cluster à partir duquel l'inertie intraclasse ne varie plus beaucoup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR8EXJX0Xy6q"
      },
      "source": [
        "# Colab crashes with this code\n",
        "# c_max = 30\n",
        "# res = []\n",
        "\n",
        "# for i in range(c_max):\n",
        "#     km = KMeans(n_clusters=i+1)\n",
        "#     km.fit(X_scaled)\n",
        "#     res.append(km.inertia_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPhNJy0CXy6s"
      },
      "source": [
        "# fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "# ax.plot(res)\n",
        "# ax.set_title(\"Représentation de l'inertie intraclasse en fonction du nombre de clusters\")\n",
        "# ax.set_xlabel(\"Nombre de clusters\")\n",
        "# ax.set_ylabel(\"Inertie intraclasse\")\n",
        "# ax.set_xticks(np.arange(c_max))\n",
        "# ax.set_xticklabels(np.arange(c_max) + 1)\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuEstcOmXy6v"
      },
      "source": [
        "Au vu du graphique ci-dessus, nous allons fixer les nombre de clusters à **9**. Cette valeur se trouve dans le coude du graphique et correspond aussi au nombre de modalités de `pnns_groups_1`.\n",
        "\n",
        "### Analyse des clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDVKTMRAXy6z"
      },
      "source": [
        "# # On réentraine le modèle avec le nombre optimal de custers\n",
        "# c_optimal = 9\n",
        "\n",
        "# # On fixe le random_state afin que l'on puisse reproduire les résultats.\n",
        "# # En effet, les centroïdes sont initialiséa aléatoirement.\n",
        "# # Modifier cette valeur pour optenir des résultats différents.\n",
        "# km = KMeans(c_optimal, random_state=0)\n",
        "# km.fit(X_scaled)\n",
        "\n",
        "# km_df = acp_df.copy()\n",
        "\n",
        "# # On ajoute les axes d'inertie trouvés lors de l'ACP\n",
        "# for i in range(4):\n",
        "#     km_df[f\"F{i + 1}\"] = X_projected[:, i]\n",
        "\n",
        "# # On attribue à chaque individu un cluster\n",
        "# km_df[\"cluster\"] = km.labels_.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgqGDnG4Xy63"
      },
      "source": [
        "# # On va prendre au hazard 200 produits pour chaque cluster.\n",
        "# # Il serait trop lourd d'afficher tous les individus sur un digramme interractif en 3D.\n",
        "# tmp = km_df.groupby(\n",
        "#     [\"cluster\"],\n",
        "#     group_keys=False\n",
        "# ).apply(lambda x: x.sample(min(len(x), 200)))\n",
        "\n",
        "# # On affiche les individus en 3D\n",
        "# fig = px.scatter_3d(\n",
        "#     tmp,\n",
        "#     x=\"F1\",\n",
        "#     y=\"F2\",\n",
        "#     z=\"F3\",\n",
        "#     color=\"cluster\",\n",
        "#     title=f\"Représentation d'un échantillon des individus projetés sur (F1, F2, F3)\"\n",
        "# )\n",
        "# fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jpFkZsSXy67"
      },
      "source": [
        "On remarque que les clusters semblent facilement identifiables même si les individus ne sont projetés que sur F1, F2 et F3.\n",
        "\n",
        "Cela peut sembler cohérent puisque notre algorithme de clusturing a utilisé les mêmes variables que celles utilisées lors de l'ACP.\n",
        "\n",
        "Observons les corrélations entre les clusters et les catégories des produits :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWX4aPjfXy69"
      },
      "source": [
        "# corr_cat_cat_heatmap(\n",
        "#     km_df,\n",
        "#     \"cluster\",\n",
        "#     \"pnns_groups_1\",\n",
        "#     \"Matrice des correlations entre les modalités des variables pnns_groups_1 et cluster\"\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0DNm51aXy7A"
      },
      "source": [
        "6 clusters semblent avoir une corrélation avec une catégorie de produit en particulier :\n",
        "- Cluster 0 : céréales et pommes de terre.\n",
        "- Cluster 1 : boissons.\n",
        "- Cluster 2 : snacks sucrés.\n",
        "- Cluster 5 : poissons, viandes et oeufs.\n",
        "- Cluster 7 : produits composés.\n",
        "- Cluster 8 : snacks salés.\n",
        "\n",
        "Observons pour chaques clusters ayant une corrélation avec une catégorie de produits, les mots qui reviennent le plus souvent dans les noms des produits :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Z3OKW0biXy7F"
      },
      "source": [
        "# def make_word_cloud(data, cluster, subplotax, title):\n",
        "#     words = data[data[\"cluster\"] == cluster][\"product_name\"].apply(lambda l: l.lower().split())\n",
        "#     cluster_words = words.apply(pd.Series).stack().reset_index(drop=True)\n",
        "#     frequencies = cluster_words.value_counts()\n",
        "    \n",
        "#     text = \" \".join(w for w in cluster_words)\n",
        "\n",
        "#     # Create and generate a word cloud image:\n",
        "#     wordcloud = WordCloud(max_font_size=40, max_words=30,\n",
        "#                           background_color=\"white\", colormap=\"magma\")\n",
        "#     wordcloud.generate_from_frequencies(frequencies)\n",
        "#     # Display the generated image:\n",
        "#     subplotax.imshow(wordcloud, interpolation='bilinear')\n",
        "#     subplotax.axis(\"off\")\n",
        "#     subplotax.set_title(title)\n",
        "#     return subplotax\n",
        "\n",
        "# tmp = km_df.dropna(subset=[\"product_name\"])\n",
        "\n",
        "# cluster_to_category = {\n",
        "#     \"0\": \"céréales et pommes de terre\",\n",
        "#     \"1\": \"boissons\",\n",
        "#     \"2\": \"snacks sucrés\",\n",
        "#     \"5\": \"poissons, viandes et oeufs\",\n",
        "#     \"7\": \"produits composés\",\n",
        "#     \"8\": \"snacks salés\",\n",
        "# }\n",
        "\n",
        "# col_nb = 2\n",
        "# row_nb = np.ceil(len(cluster_to_category) / col_nb).astype(int)\n",
        "\n",
        "# fig = plt.figure(figsize=(16, 5 * row_nb))\n",
        "\n",
        "# for i, (cluster, category) in enumerate(cluster_to_category.items()):\n",
        "#     ax = plt.subplot(row_nb, col_nb, i + 1)\n",
        "    \n",
        "#     make_word_cloud(tmp, cluster, ax, f\"Cluster {cluster} corrélé aux {category}\")\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVxK6qQsXy7M"
      },
      "source": [
        "On voit effectivement que la plupart des mots représentent bien la catégorie de produit avec laquelle le cluster est corrélé.\n",
        "\n",
        "Nous avons utilisé toutes les variables disponibles dans notre jeu de données nettoyé pour la recherche de 9 groupes de produits. Cette méthode pourrait permettre aux agents de Santé publique France de rechercher automatiquement d'autres groupes de produits selon des critères spécifiques. Par exemple, on pourrait ne prendre en compte qu'une poignée de variables et une catégorie spécifique de produits etc..."
      ]
    }
  ]
}
